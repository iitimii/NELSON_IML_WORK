{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple implementation of LSTM from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "import lightning as L\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mKernel nei (Python 3.8.13) is not usable. Check the Jupyter output tab for more information. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# A simple LSTM cell\n",
    "from IPython import display\n",
    "display.Image(\"lstm.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the LSTM NN class\n",
    "\n",
    "class tiny_lstm(L.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super().__init__()\n",
    "        #create and initialize the weights and biases using a normal distribution\n",
    "        mean = torch.tensor(0.0)\n",
    "        std = torch.tensor(1.0)\n",
    "\n",
    "        #percent of long term mem to remember\n",
    "        self.h_f = nn.Parameter(torch.normal(mean = mean, std =std), requires_grad=True)\n",
    "        self.iw_f = nn.Parameter(torch.normal(mean = mean, std =std), requires_grad=True)\n",
    "        self.b_f= nn.Parameter(torch.tensor(0.), requires_grad=True)\n",
    "\n",
    "        #percent of potential memory to remember\n",
    "        self.h_pr = nn.Parameter(torch.normal(mean = mean, std =std), requires_grad=True)\n",
    "        self.iw_pr= nn.Parameter(torch.normal(mean = mean, std =std), requires_grad=True)\n",
    "        self.b_pr = nn.Parameter(torch.tensor(0.), requires_grad=True)\n",
    "\n",
    "        #potential memory\n",
    "        self.h_p = nn.Parameter(torch.normal(mean = mean, std =std), requires_grad=True)\n",
    "        self.iw_p = nn.Parameter(torch.normal(mean = mean, std =std), requires_grad=True)\n",
    "        self.b_p = nn.Parameter(torch.tensor(0.), requires_grad=True)\n",
    "        \n",
    "        #output\n",
    "        self.h_o = nn.Parameter(torch.normal(mean = mean, std =std), requires_grad=True)\n",
    "        self.iw_o = nn.Parameter(torch.normal(mean = mean, std =std), requires_grad=True)\n",
    "        self.b_o = nn.Parameter(torch.tensor(0.), requires_grad=True)\n",
    "\n",
    "    def lstm_units(self, input_value, longmem_value, shortmem_value):\n",
    "\n",
    "        long_rem_percent = torch.sigmoid((input_value*self.iw_f)+ (shortmem_value*self.h_f) + self.b_f)\n",
    "        potential_rem_percent = torch.sigmoid((input_value*self.iw_pr)+(shortmem_value*self.h_pr)+self.b_pr)\n",
    "        potential_mem = torch.tanh((input_value*self.iw_p)+ (shortmem_value*self.h_p) + self.b_p)\n",
    "\n",
    "        update_long_mem = (longmem_value * long_rem_percent) + (potential_rem_percent * potential_mem)\n",
    "        output_rem_percent =  torch.sigmoid((input_value*self.iw_o)+(shortmem_value*self.h_o)+self.b_o)\n",
    "\n",
    "        update_short_mem = torch.tanh(update_long_mem) * output_rem_percent\n",
    "\n",
    "        return ([update_long_mem, update_short_mem])\n",
    "\n",
    "    def forward(self, input):\n",
    "        long_mem = 0\n",
    "        short_mem = 0\n",
    "        for i in range(len(input)):\n",
    "            long_mem, short_mem = self.lstm_units(input[i], long_mem, short_mem)\n",
    "        return short_mem #as the output\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return Adam(self.parameters(), lr = 0.001)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        #batch contains the different data from the two companies\n",
    "        input_i, label_i = batch\n",
    "        output_i = self.forward(input_i[0])\n",
    "        loss = (output_i - label_i)**2\n",
    "        self.log(\"train loss\", loss)\n",
    "\n",
    "        if (label_i == 0):\n",
    "            self.log(\"out_0\", output_i) \n",
    "        else: \n",
    "            self.log(\"out_1\", output_i)\n",
    "        return loss\n",
    "                     \n",
    "    # To plot the logs in Tensorboard, type tensorboard --logdir=lightning_logs/ in the directory of the log file created by lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the LSTM\n",
    "model = tiny_lstm()\n",
    "\n",
    "inputs = torch.tensor([[0., 5, .25, 1.], [1., .5, .25, 1.]])\n",
    "labels = torch.tensor([0., 1.])\n",
    "\n",
    "dataset = TensorDataset(inputs, labels)\n",
    "dataloader = DataLoader(dataset)\n",
    "\n",
    "trainer = L.Trainer(max_epochs=2000)\n",
    "trainer.fit(model, train_dataloaders = dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mKernel nei (Python 3.8.13) is not usable. Check the Jupyter output tab for more information. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\"\"\" # why we don't need to implicity call the forward method\n",
    "class Module:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self, data):\n",
    "        self.forward(data)\n",
    "\n",
    "    def forward(self, data):\n",
    "        print(\"forward function, data =\", data)\n",
    "\n",
    "net = Module()\n",
    "net([1,2,3])\n",
    "# forward function, data = [1, 2, 3]\n",
    "Now that we have out Module class, let's create another Net class that inherits from it\n",
    "\n",
    "# Net inherits from Module\n",
    "class Net(Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "    def forward(self, data):\n",
    "        print(\"Net.forward, data =\", data)\n",
    "\n",
    "net = Net()\n",
    "net([1,2,3,4])\n",
    "# Net.forward, data = [1, 2, 3, 4] \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Pytorch's LSTM module\n",
    "class tiny_lstm_pytorch(L.LightningModule):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(tiny_lstm_pytorch, self).__init()\n",
    "        self.lstm = nn.LSTM(input_size =1, hidden_size=1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        input_trans = input.view(len(input), 1)\n",
    "        lstm_out, temp = self.lstm(input_trans)\n",
    "        prediction = lstm_out[-1]\n",
    "        return prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nei",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "173c0cb41f479ae2d1f90bf66f9ae3aceca0c8feada6413b4ebace4131a19a6b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
