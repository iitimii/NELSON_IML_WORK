{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file and specific characters in the file\n",
    "\n",
    "data = open('bless.txt', 'r').read()\n",
    "char = list(set(data))\n",
    "\n",
    "char_size, data_size = len(char), len(data)\n",
    "char_size, data_size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the vocab size, convert the chars to vectors\n",
    "char_to_int = { ch:i for i,ch in enumerate(char)}\n",
    "int_to_char = { i:ch for i,ch in enumerate(char)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one-hot encode all characters\n",
    "char_vector = np.zeros((np.int32(char_size), np.int32(char_size)))\n",
    " \n",
    "for x, y in int_to_char.items():\n",
    "    char_vector[x, char_to_int[y]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model architecture, weights and biases\n",
    "\n",
    "#hyperparameters\n",
    "hidden_size = 100 #-> hundred neuron for its hidden layer\n",
    "seq_len = 25\n",
    "lr = 1e-1\n",
    "\n",
    "#weights\n",
    "wx = np.random.randn(hidden_size, char_size) #for the sake of dot multiplication, it's meant to be (char_size, hidden_size)\n",
    "whh = np.random.randn(hidden_size, hidden_size)  \n",
    "who = np.random.randn (char_size, hidden_size) #for the sake of dot multiplication, it's meant to be (hidden_size, char_size)\n",
    "\n",
    "#biases\n",
    "bh = np.zeros((hidden_size, 1))\n",
    "by = np.zeros((char_size, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the loss function\n",
    "\n",
    "#we want to implement the forward and backward pass\n",
    "\n",
    "def lossfun(inputs, target, prevh):\n",
    "\n",
    "    #store the input characters, hidden states, target values and the probability of the output at every time steps\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "\n",
    "    #save the previous hidden state\n",
    "    hs[-1] = np.copy(prevh) \n",
    "    loss = 0\n",
    "\n",
    "    #forward pass\n",
    "    for i in range(len(inputs)):\n",
    "        # for every input (character), get it encoded form\n",
    "        xs[i] = char_vector[char_to_int[inputs[i]]]\n",
    "        xs[i] = xs[i].reshape(char_size,1)\n",
    "\n",
    "        # calc the hidden state\n",
    "        hs[i] = np.tanh(np.dot(wx, xs[i]) + np.dot(whh, hs[i-1]) + bh) \n",
    "\n",
    "        # unnormalized log probabilities for next chars\n",
    "        ys[i] = np.dot(who, hs[i]) + by \n",
    "        # probabilities for next chars\n",
    "        ps[i] = np.exp(ys[i]) / np.sum(np.exp(ys[i])) \n",
    "        \n",
    "        # get the integer value of the character and find \n",
    "        # where ps[i] == the target value == 1 in the one hot encoded form\n",
    "        \n",
    "        loss += -np.log(ps[i][target[i],0]) # softmax (cross-entropy loss)\n",
    "  \n",
    "    #backward pass      \n",
    "    #initialize the gradients going backwards\n",
    "    dwx, dwhh, dwho = np.zeros_like(wx), np.zeros_like(whh), np.zeros_like(who)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dhnext = np.zeros_like(hs[0]) \n",
    "\n",
    "    #calculate the gradient going backwards\n",
    "    for i in reversed(range(len(inputs))):\n",
    "\n",
    "        dy = np.copy(ps[i])\n",
    "\n",
    "        #say E is the error\n",
    "        #dE/dy where y is the output not the softmax of the output\n",
    "        dy[target[i]] -= 1 # backprop into y. \n",
    "\n",
    "        #dE/dwho = dE/dy*(dy/dwho --> h = dy/dwho)\n",
    "        dwho += np.dot(dy, hs[i].T)\n",
    "\n",
    "        #dE/dby \n",
    "        dby += dy\n",
    "\n",
    "        #dE/dh = dE/dy*(dy/dh --> dwho ) + dhnext \n",
    "        dh = np.dot(who.T, dy) + dhnext \n",
    "\n",
    "        # backprop through tanh nonlinearity\n",
    "        dhraw = (1 - hs[i] * hs[i]) * dh \n",
    "        dbh += dhraw\n",
    "\n",
    "        #dE/dwx = ...\n",
    "        dwx += np.dot(dhraw, xs[i].T)\n",
    "        dwhh += np.dot(dhraw, hs[i-1].T)\n",
    "        dhnext = np.dot(whh.T, dhraw)\n",
    "\n",
    "    for dparam in [dwx, dwhh, dwho, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "    return loss, dwx, dwhh, dwho, dbh, dby, hs[len(inputs)-1]\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "  '''                                                                                                                                                                                        \n",
    "  sample a sequence of integers from the model                                                                                                                                                \n",
    "  h is memory state, seed_ix is seed letter for first time step   \n",
    "  n is how many characters to predict\n",
    "  \n",
    "  '''\n",
    "\n",
    "  x = char_vector[char_to_int[seed_ix]]\n",
    "  #list to store generated chars\n",
    "  x = x.reshape(1,x.shape[0])\n",
    "  x = x.T\n",
    "  ixes = []\n",
    "  #for as many characters as we want to generate\n",
    "  for i in range(n):\n",
    "    #a hidden state at a given time step is a function \n",
    "    #of the input at the same time step modified by a weight matrix \n",
    "    #added to the hidden state of the previous time step \n",
    "    #multiplied by its own hidden state to hidden state matrix.\n",
    "    h = np.tanh(np.dot(wx, x) + np.dot(whh, h) + bh)\n",
    "    #compute output (unnormalised)\n",
    "    y = np.dot(who, h) + by\n",
    "    ## probabilities for next chars\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "\n",
    "    #pick one with the highest probability \n",
    "    ix = np.random.choice(range(char_size), p=p.ravel())\n",
    "    #create a vector\n",
    "    x = np.zeros((char_size, 1))\n",
    "    #customize it for the predicted char\n",
    "    x[ix] = 1\n",
    "    #add it to the list\n",
    "    ixes.append(ix)\n",
    "\n",
    "  txt = ''.join(int_to_char[ix] for ix in ixes)\n",
    "  print ('----\\n %s \\n----' % (txt, ))\n",
    "hprev = np.zeros((hidden_size,1)) # reset RNN memory  \n",
    "#predict the 200 next characters given 'a'\n",
    "sample(hprev, 'a', 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" inh = \"Mmel\"\n",
    "target_list = []\n",
    "target = \"mmel\"\n",
    "for element in \"mmel\":\n",
    "   target_list.append(char_to_int[element])\n",
    "\n",
    "hprev = np.zeros((hidden_size,1))\n",
    "\n",
    "lossfun(inh, target_list, hprev) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(wx), np.zeros_like(whh), np.zeros_like(who)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad                                                                                                                \n",
    "smooth_loss = -np.log(1.0/char_size)*seq_len # loss at iteration 0                                                                                                                        \n",
    "while n<=10000:\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  # check \"How to feed the loss function to see how this part works\n",
    "  if p+seq_len+1 >= len(data) or n == 0:\n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory                                                                                                                                      \n",
    "    p = 0 # go from start of data   \n",
    "\n",
    "  inputs = [ch for ch in data[p:p+seq_len]]\n",
    "  targets = [char_to_int[ch] for ch in data[p+1:p+seq_len+1]]\n",
    "  # forward seq_length characters through the net and fetch gradient                                                                                                                          \n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossfun(inputs, targets, hprev)\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "  # sample from the model now and then                                                                                                                                                        \n",
    "  if n % 10 == 0:\n",
    "    print ('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "    sample(hprev, inputs[0], 200)\n",
    "\n",
    "  # perform parameter update with Adagrad                                                                                                                                                     \n",
    "  for param, dparam, mem in zip([wx, whh, who, bh, by],\n",
    "                                [dWxh, dWhh, dWhy, dbh, dby],\n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "    mem += dparam * dparam\n",
    "    param += -lr * dparam / np.sqrt(mem + 1e-8) # adagrad update                                                                                                                   \n",
    "\n",
    "  p += seq_len # move data pointer                                                                                                                                                         \n",
    "  n += 1 # iteration counter    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nei",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "173c0cb41f479ae2d1f90bf66f9ae3aceca0c8feada6413b4ebace4131a19a6b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
