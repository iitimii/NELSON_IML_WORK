{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Timii\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias, device):\n",
    "        super(ConvLSTMcell, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.bias = bias\n",
    "        self.device = device\n",
    "        self.padding = kernel_size[0]//2, kernel_size[1]//2\n",
    "        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
    "        out_channels=4*self.hidden_dim, kernel_size=self.kernel_size, padding=self.padding, bias=self.bias)\n",
    "\n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        h_cur, c_cur = cur_state\n",
    "        combined = torch.cat([input_tensor, h_cur], dim=1)  # concatenate along channel axis\n",
    "        combined_conv = self.conv(combined)\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "        return h_next, c_next\n",
    "\n",
    "    def init_hidden(self, batch_size, image_size):\n",
    "        height, width = image_size\n",
    "        return (torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device).to(self.device),\n",
    "                torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device).to(self.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, features):\n",
    "        super(Model, self).__init__()\n",
    "        self.features = features\n",
    "        self.device = features[\"device\"]\n",
    "        self.context_frames = features[\"n_past\"]\n",
    "        self.n_future = features[\"n_future\"]\n",
    "        self.model_dir = features[\"model_dir\"]\n",
    "        self.model_name_save_appendix = features[\"model_name_save_appendix\"]\n",
    "        self.convlstm1 = ConvLSTMCell(input_dim=524, hidden_dim=524, kernel_size=(3, 3), bias=True, device=self.device).to(self.device)\n",
    "        self.convlstm2 = ConvLSTMCell(input_dim=524, hidden_dim=524, kernel_size=(3, 3), bias=True, device=self.device).cuda()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5, stride=1, padding=2).cuda()\n",
    "        self.conv12 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=2).cuda()\n",
    "        self.conv23 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5, stride=1, padding=2).cuda()\n",
    "        self.conv34 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=5, stride=1, padding=2).cuda()\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=3, kernel_size=5, stride=1, padding=2).cuda()\n",
    "        self.conv3 = nn.Conv2d(in_channels=48, out_channels=3, kernel_size=5, stride=1, padding=2).cuda()\n",
    "        self.upconv1 = nn.Conv2d(in_channels=524, out_channels=256, kernel_size=5, stride=1, padding=2).cuda()\n",
    "        # self.upconv2 = nn.Conv2d(in_channels=512, out_channels=256, kernel_size=5, stride=1, padding=2).cuda()\n",
    "        self.upconv2 = nn.Conv2d(in_channels=256, out_channels=128, kernel_size=5, stride=1, padding=2).cuda()\n",
    "        self.upconv3 = nn.Conv2d(in_channels=128, out_channels=64, kernel_size=5, stride=1, padding=2).cuda()\n",
    "        self.upconv4 = nn.Conv2d(in_channels=64, out_channels=3, kernel_size=5, stride=1, padding=2).cuda()\n",
    "        self.maxpool1 = nn.MaxPool2d(2, stride=2)\n",
    "        self.maxpool2 = nn.MaxPool2d(2, stride=4)\n",
    "        self.relu1 = nn.ReLU().to(self.device)\n",
    "        self.relu2 = nn.ReLU().to(self.device)\n",
    "        self.upsample1 = nn.Upsample(scale_factor=1)\n",
    "        self.upsample2 = nn.Upsample(scale_factor=2)\n",
    "        self.upsample3 = nn.Upsample(scale_factor=4)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        # if self.optimizer == \"adam\" or self.optimizer == \"Adam\":\n",
    "        #     self.optimizer = optim.Adam(Model.parameters(), lr=learning_rate)\n",
    "\n",
    "        self.criterion = features[\"criterion\"]\n",
    "        if self.criterion == \"L1\":\n",
    "            self.mae_criterion = nn.L1Loss()\n",
    "        if self.criterion == \"L2\":\n",
    "            self.mae_criterion = nn.MSELoss()\n",
    "\n",
    "    # def save_model(self):\n",
    "    #     torch.save(self.model.state_dict(), self.model_dir + \"ATCVP_model\" + self.model_name_save_appendix)\n",
    "\n",
    "    def run(self, scene, actions, touch, test=False):\n",
    "        mae = 0\n",
    "\n",
    "        # self.optimizer.zero_grad()\n",
    "        self.batch_size = actions.shape[1]  # 1\n",
    "        state = actions[0]  # 0\n",
    "        batch_size__ = scene.shape[1]  # 1\n",
    "        hidden_1, cell_1 = self.convlstm1.init_hidden(batch_size=self.batch_size, image_size=(8, 8))\n",
    "        hidden_2, cell_2 = self.convlstm2.init_hidden(batch_size=self.batch_size, image_size=(8, 8))\n",
    "        # Initialize output\n",
    "        outputs = []\n",
    "        for index, (sample_scene, sample_action, sample_touch) in enumerate(zip(scene[0:-1].squeeze(), actions[1:].squeeze(), touch[1:].squeeze())):\n",
    "            # 2. Run through lstm:\n",
    "            if index > self.context_frames - 1:\n",
    "                # DOWNSAMPLING\n",
    "                # Scene Downsampling\n",
    "                out_scene1 = self.maxpool1(self.relu1(self.conv1(output).float()))\n",
    "                out_scene2 = self.maxpool1(self.relu1(self.conv12(out_scene1.float())))\n",
    "                out_scene3 = self.maxpool2(self.relu1(self.conv23(out_scene2.float())))\n",
    "                out_scene4 = self.maxpool1(self.relu1(self.conv34(out_scene3.float())))\n",
    "                # Touch Downsampling\n",
    "                out_touch1 = self.maxpool1(self.relu1(self.conv1(sample_touch).float()))\n",
    "                out_touch2 = self.maxpool1(self.relu1(self.conv12(out_touch1.float())))\n",
    "                out_touch3 = self.maxpool2(self.relu1(self.conv23(out_touch2.float())))\n",
    "                out_touch4 = self.maxpool1(self.relu1(self.conv34(out_touch3.float())))\n",
    "\n",
    "                state_action = torch.cat((state, sample_action), 1)\n",
    "                # Adding Touch to Scene\n",
    "                scene_and_touch = torch.cat((out_scene4, out_touch4), 1)\n",
    "                # Adding Actions to Scene + Touch\n",
    "                robot_and_scene_and_touch = torch.cat((torch.cat(8 * [torch.cat(8 * [state_action.unsqueeze(2)], axis=2).unsqueeze(3)], axis=3), scene_and_touch.squeeze()), 1)\n",
    "                # LSTM Chain\n",
    "                hidden_1, cell_1 = self.convlstm1(input_tensor=robot_and_scene_and_touch.float(),\n",
    "                                                  cur_state=[hidden_1, cell_1])\n",
    "                hidden_2, cell_2 = self.convlstm2(input_tensor=hidden_1, cur_state=[hidden_2, cell_2])\n",
    "                # UPSAMPLING\n",
    "                up1 = self.upsample2(self.relu2(self.upconv1(hidden_2)))\n",
    "                up2 = self.upsample2(self.relu2(self.upconv2(up1)))\n",
    "                up3 = self.upsample2(self.relu2(self.upconv3(up2)))\n",
    "                up4 = self.upsample3(self.relu2(self.upconv4(up3)))\n",
    "                skip_connection_added = torch.cat((up4, output.float()), 1)\n",
    "                output = self.conv2(skip_connection_added)\n",
    "                output = self.tanh(output)\n",
    "\n",
    "                mae += self.mae_criterion(output, scene[index + 1])\n",
    "\n",
    "                outputs.append(output)\n",
    "\n",
    "            else:\n",
    "                # DOWNSAMPLING\n",
    "                # Scene Downsampling\n",
    "                out_scene1 = self.maxpool1(self.relu1(self.conv1(sample_scene).float()))\n",
    "                out_scene2 = self.maxpool1(self.relu1(self.conv12(out_scene1.float())))\n",
    "                out_scene3 = self.maxpool2(self.relu1(self.conv23(out_scene2.float())))\n",
    "                out_scene4 = self.maxpool1(self.relu1(self.conv34(out_scene3.float())))\n",
    "                # Touch Downsampling\n",
    "                out_touch1 = self.maxpool1(self.relu1(self.conv1(sample_touch).float()))\n",
    "                out_touch2 = self.maxpool1(self.relu1(self.conv12(out_touch1.float())))\n",
    "                out_touch3 = self.maxpool2(self.relu1(self.conv23(out_touch2.float())))\n",
    "                out_touch4 = self.maxpool1(self.relu1(self.conv34(out_touch3.float())))\n",
    "\n",
    "                state_action = torch.cat((state, sample_action), 1)\n",
    "                # Adding Touch to Scene\n",
    "                scene_and_touch = torch.cat((out_scene4, out_touch4), 1)\n",
    "                # Adding Actions to Scene + Touch\n",
    "                robot_and_scene_and_touch = torch.cat((torch.cat(8 * [torch.cat(8 * [state_action.unsqueeze(2)], axis=2).unsqueeze(3)], axis=3), scene_and_touch.squeeze()), 1)\n",
    "                # LSTM Chain\n",
    "                hidden_1, cell_1 = self.convlstm1(input_tensor=robot_and_scene_and_touch.float(), cur_state=[hidden_1, cell_1])\n",
    "                hidden_2, cell_2 = self.convlstm2(input_tensor=hidden_1, cur_state=[hidden_2, cell_2])\n",
    "                # UPSAMPLING\n",
    "                up1 = self.upsample2(self.relu2(self.upconv1(hidden_2)))\n",
    "                up2 = self.upsample2(self.relu2(self.upconv2(up1)))\n",
    "                up3 = self.upsample2(self.relu2(self.upconv3(up2)))\n",
    "                up4 = self.upsample3(self.relu2(self.upconv4(up3)))\n",
    "                skip_connection_added = torch.cat((up4, sample_scene.float()), 1)\n",
    "                output = self.conv2(skip_connection_added)\n",
    "\n",
    "                mae += self.mae_criterion(output, scene[index + 1])\n",
    "\n",
    "                last_output = output\n",
    "\n",
    "        outputs = [last_output] + outputs\n",
    "\n",
    "        if test is False:\n",
    "            loss = mae\n",
    "            loss.backward()\n",
    "\n",
    "            # self.optimizer.step()\n",
    "\n",
    "        return mae.data.cpu().numpy() / (self.context_frames + self.n_future), torch.stack(outputs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {'device':'cpu', \n",
    "            'n_past': 1,\n",
    "            'n_future': 1,\n",
    "            'model_dir': 'models',\n",
    "            'model_name_save_appendix': 'test'}\n",
    "\n",
    "t_model = Model(features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "947f030b3e678118fc438144c1e47ca5c23949e6feee86165ca58c1240ce2eba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
