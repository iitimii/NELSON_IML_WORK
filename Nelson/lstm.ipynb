{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn.functional as F  # Parameterless functions, like (some) activation functions\n",
    "import torchvision.datasets as datasets  # Standard datasets\n",
    "import torchvision.transforms as transforms  # Transformations we can perform on our dataset for augmentation\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset  # Gives easier dataset managment by creating mini batches etc.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, f1_score, recall_score, precision_score\n",
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torch.autograd import Variable \n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the data \n",
    "#algorithm to read all the files\n",
    "\n",
    "'''\n",
    "for folder in this folder:\n",
    "    read xelasensor1.csv\n",
    "    read sliplabel.csv\n",
    "    concat it in a single dataframe along axis = 0\n",
    "\n",
    "print the dataframe\n",
    "'''\n",
    "\n",
    "directory = '/Users/elijahnelson/Desktop/SIWES/IML/Tactile_IML/train2dof'\n",
    "directory2 = '/Users/elijahnelson/Desktop/SIWES/IML/Tactile_IML/'\n",
    "\n",
    "def read_file(detect_or_pred, n = None):\n",
    "\n",
    "    #store all directories in a list\n",
    "    list_xela_allfiles = []\n",
    "    list_sliplabel_allfiles = []\n",
    "\n",
    "    for root, subdirectories, files in os.walk(directory):\n",
    "        for sdirectory in subdirectories:\n",
    "\n",
    "            #subdirectory with absolute path\n",
    "            subdirectory = '{}/{}'.format(root, sdirectory)\n",
    "\n",
    "            #read specific files in the subdirectory\n",
    "            for file in os.listdir(subdirectory):\n",
    "            \n",
    "                if file.endswith(\"sensor1.csv\"):\n",
    "                    df = pd.read_csv('{}/{}'.format(subdirectory, file), index_col=None, header=0)\n",
    "                    \n",
    "                    if detect_or_pred ==0:\n",
    "                        list_xela_allfiles.append(df)\n",
    "                    elif detect_or_pred ==1 and n is not None:\n",
    "                        list_xela_allfiles.append(df[:-n])\n",
    "\n",
    "                if file.endswith(\"label.csv\"):\n",
    "                    df = pd.read_csv('{}/{}'.format(subdirectory, file), index_col=None, header=0)\n",
    "                    if detect_or_pred ==0:\n",
    "                        list_sliplabel_allfiles.append(df)\n",
    "                    elif detect_or_pred ==1 and n is not None: \n",
    "                        list_sliplabel_allfiles.append(df[n:])\n",
    "\n",
    "    return list_xela_allfiles, list_sliplabel_allfiles\n",
    "\n",
    "    #np.newaxis; np.zeros (3,4,4) -> \n",
    "                    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_metrics(xela_test, sliplabel_test):\n",
    "    #predict using the holdout set (DONE)\n",
    "    predicted_cls = xela_test\n",
    "\n",
    "    #Plot the loss values against number of epochs (DONE)\n",
    "    #validation test (DONE)\n",
    "\n",
    "    #Print the accuracy\n",
    "    x = 0\n",
    "    for i in range(predicted_cls.shape[0]):\n",
    "        if predicted_cls[i, 0].item() == sliplabel_test[i, 0]:\n",
    "            x += 1\n",
    "\n",
    "    accuracy = x/ float(sliplabel_test.shape[0])\n",
    "    print(f'Accuracy for slip detection is {accuracy}')\n",
    "\n",
    "    #Print the fscore\n",
    "    fscore = f1_score(sliplabel_test, predicted_cls, average='macro')\n",
    "    print(f'Fscore for slip detection is {fscore}')\n",
    "\n",
    "    #print the Precision\n",
    "    precision = precision_score(sliplabel_test, predicted_cls, average='macro')\n",
    "    print(f'Precision for slip detection is {precision}')\n",
    "\n",
    "    #print the Recall\n",
    "    recall = recall_score(sliplabel_test, predicted_cls, average='macro')\n",
    "    print(f'Recall for slip detection is {recall}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# READ DATA FROM FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat the list of xela_allfiles and sliplabel_allfiles across axis = 0\n",
    "n = 5\n",
    "list_xela_allfiles, list_sliplabel_allfiles = read_file(0)\n",
    "\n",
    "#for slip prediction, comment the line above and uncomment the line below\n",
    "#list_xela_allfiles, list_sliplabel_allfiles = read_file(1, n)\n",
    "\n",
    "pd_xela_allfiles = pd.concat(list_xela_allfiles, axis=0, ignore_index=True)\n",
    "pd_sliplabel_allfiles = pd.concat(list_sliplabel_allfiles, axis=0, ignore_index=True)\n",
    "pd_sliplabel_allfiles = pd_sliplabel_allfiles['slip']\n",
    "\n",
    "#reshape the target array into (rows, 1)\n",
    "tac_label = pd_sliplabel_allfiles.values.reshape(pd_sliplabel_allfiles.shape[0], 1)\n",
    "\n",
    "sc = StandardScaler()\n",
    "pd_xela_allfiles = sc.fit_transform(pd_xela_allfiles)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA SPLITTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([160746, 10, 48]), torch.Size([160746, 1]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split into train and validation\n",
    "tac_train, tac_valid, tac_label_train, tac_label_valid = train_test_split(pd_xela_allfiles, tac_label, test_size=0.3, shuffle = True)\n",
    "\n",
    "#convert to numpy values\n",
    "data = tac_train#.to_numpy()\n",
    "data_valid = tac_valid#.to_numpy()\n",
    "\n",
    "labels = tac_label_train\n",
    "labels_valid = tac_label_valid\n",
    "\n",
    "#Arrange the data according to the desired sequence length\n",
    "seq = 10\n",
    "x = []\n",
    "x_valid = []\n",
    "for t in range(data.shape[0]-(seq-1)):\n",
    "    x.append(data[t:t+seq])\n",
    "\n",
    "for t in range(data_valid.shape[0]-(seq-1)):\n",
    "    x_valid.append(data_valid[t:t+seq])\n",
    "\n",
    "x = np.array(x)\n",
    "x_valid = np.array(x_valid)\n",
    "\n",
    "x_train = Variable(torch.Tensor(x))\n",
    "x_train = torch.reshape(x_train,   (x_train.shape[0], seq, 48))\n",
    "\n",
    "x_valid = Variable(torch.Tensor(x_valid))\n",
    "x_valid = torch.reshape(x_valid,   (x_valid.shape[0], seq, 48))\n",
    "\n",
    "y = labels[seq-1:]\n",
    "y_valid = labels_valid[seq-1:]\n",
    "\n",
    "y_train = Variable(torch.Tensor(y))\n",
    "y_valid = Variable(torch.Tensor(y_valid))\n",
    "\n",
    "pd_xela_allfiles = Variable(torch.Tensor(pd_xela_allfiles))\n",
    "tac_label = Variable(torch.Tensor(tac_label))\n",
    "\n",
    "\n",
    "x_train.shape, y_train.shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEFINE THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the model architecture\n",
    "\n",
    "class LSTM1(nn.Module):\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers, seq_length):\n",
    "        super(LSTM1, self).__init__()\n",
    "        self.num_classes = num_classes \n",
    "        self.num_layers = num_layers \n",
    "        self.input_size = input_size \n",
    "        self.hidden_size = hidden_size \n",
    "        self.seq_length = seq_length \n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                          num_layers=num_layers, batch_first=True) \n",
    "        self.fc_1 =  nn.Linear(hidden_size, 32)\n",
    "        self.fc = nn.Linear(32, num_classes) \n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) \n",
    "        c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) \n",
    "        \n",
    "        output, (hn, cn) = self.lstm(x, (h_0, c_0)) \n",
    "        hn = hn.view(-1, self.hidden_size)\n",
    "        out = self.relu(hn)\n",
    "        out = self.fc_1(out) \n",
    "        out = self.relu(out) \n",
    "        out = self.fc(out) \n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "        \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For training epoch 1, loss =0.70213997 For validation epoch 1, loss =0.69640702\n",
      "For training epoch 2, loss =0.69837016 For validation epoch 2, loss =0.69270575\n",
      "For training epoch 3, loss =0.69469184 For validation epoch 3, loss =0.68910354\n",
      "For training epoch 4, loss =0.69109124 For validation epoch 4, loss =0.68559492\n",
      "For training epoch 5, loss =0.68755829 For validation epoch 5, loss =0.68213952\n",
      "For training epoch 6, loss =0.68409085 For validation epoch 6, loss =0.67874402\n",
      "For training epoch 7, loss =0.68068004 For validation epoch 7, loss =0.67541200\n",
      "For training epoch 8, loss =0.67732245 For validation epoch 8, loss =0.67213905\n",
      "For training epoch 9, loss =0.67400914 For validation epoch 9, loss =0.66891563\n",
      "For training epoch 10, loss =0.67073840 For validation epoch 10, loss =0.66572636\n",
      "For training epoch 11, loss =0.66750240 For validation epoch 11, loss =0.66257489\n",
      "For training epoch 12, loss =0.66429597 For validation epoch 12, loss =0.65945286\n",
      "For training epoch 13, loss =0.66111988 For validation epoch 13, loss =0.65635651\n",
      "For training epoch 14, loss =0.65796942 For validation epoch 14, loss =0.65328681\n",
      "For training epoch 15, loss =0.65484166 For validation epoch 15, loss =0.65024149\n",
      "For training epoch 16, loss =0.65173155 For validation epoch 16, loss =0.64721459\n",
      "For training epoch 17, loss =0.64863992 For validation epoch 17, loss =0.64420176\n",
      "For training epoch 18, loss =0.64556450 For validation epoch 18, loss =0.64120096\n",
      "For training epoch 19, loss =0.64250445 For validation epoch 19, loss =0.63821000\n",
      "For training epoch 20, loss =0.63946038 For validation epoch 20, loss =0.63523352\n",
      "For training epoch 21, loss =0.63642901 For validation epoch 21, loss =0.63227105\n",
      "For training epoch 22, loss =0.63340878 For validation epoch 22, loss =0.62931740\n",
      "For training epoch 23, loss =0.63040471 For validation epoch 23, loss =0.62637585\n",
      "For training epoch 24, loss =0.62741685 For validation epoch 24, loss =0.62344909\n",
      "For training epoch 25, loss =0.62444621 For validation epoch 25, loss =0.62054479\n",
      "For training epoch 26, loss =0.62149251 For validation epoch 26, loss =0.61766326\n",
      "For training epoch 27, loss =0.61855620 For validation epoch 27, loss =0.61481267\n",
      "For training epoch 28, loss =0.61563396 For validation epoch 28, loss =0.61198568\n",
      "For training epoch 29, loss =0.61272317 For validation epoch 29, loss =0.60917360\n",
      "For training epoch 30, loss =0.60982788 For validation epoch 30, loss =0.60636616\n",
      "For training epoch 31, loss =0.60694426 For validation epoch 31, loss =0.60356897\n",
      "For training epoch 32, loss =0.60406756 For validation epoch 32, loss =0.60078782\n",
      "For training epoch 33, loss =0.60119802 For validation epoch 33, loss =0.59802741\n",
      "For training epoch 34, loss =0.59833866 For validation epoch 34, loss =0.59527558\n",
      "For training epoch 35, loss =0.59548330 For validation epoch 35, loss =0.59251797\n",
      "For training epoch 36, loss =0.59263033 For validation epoch 36, loss =0.58975399\n",
      "For training epoch 37, loss =0.58978570 For validation epoch 37, loss =0.58699000\n",
      "For training epoch 38, loss =0.58694416 For validation epoch 38, loss =0.58423120\n",
      "For training epoch 39, loss =0.58410615 For validation epoch 39, loss =0.58147621\n",
      "For training epoch 40, loss =0.58127421 For validation epoch 40, loss =0.57871699\n",
      "For training epoch 41, loss =0.57844526 For validation epoch 41, loss =0.57595086\n",
      "For training epoch 42, loss =0.57561523 For validation epoch 42, loss =0.57318640\n",
      "For training epoch 43, loss =0.57278526 For validation epoch 43, loss =0.57042491\n",
      "For training epoch 44, loss =0.56995749 For validation epoch 44, loss =0.56766856\n",
      "For training epoch 45, loss =0.56713217 For validation epoch 45, loss =0.56491143\n",
      "For training epoch 46, loss =0.56430411 For validation epoch 46, loss =0.56214511\n",
      "For training epoch 47, loss =0.56146729 For validation epoch 47, loss =0.55936760\n",
      "For training epoch 48, loss =0.55861795 For validation epoch 48, loss =0.55657202\n",
      "For training epoch 49, loss =0.55575103 For validation epoch 49, loss =0.55374497\n",
      "For training epoch 50, loss =0.55285954 For validation epoch 50, loss =0.55088973\n",
      "For training epoch 51, loss =0.54994136 For validation epoch 51, loss =0.54801422\n",
      "For training epoch 52, loss =0.54699099 For validation epoch 52, loss =0.54509789\n",
      "For training epoch 53, loss =0.54400140 For validation epoch 53, loss =0.54213959\n",
      "For training epoch 54, loss =0.54096740 For validation epoch 54, loss =0.53913254\n",
      "For training epoch 55, loss =0.53788155 For validation epoch 55, loss =0.53606892\n",
      "For training epoch 56, loss =0.53473729 For validation epoch 56, loss =0.53292823\n",
      "For training epoch 57, loss =0.53152686 For validation epoch 57, loss =0.52970368\n",
      "For training epoch 58, loss =0.52823842 For validation epoch 58, loss =0.52639508\n",
      "For training epoch 59, loss =0.52486223 For validation epoch 59, loss =0.52299196\n",
      "For training epoch 60, loss =0.52138197 For validation epoch 60, loss =0.51947075\n",
      "For training epoch 61, loss =0.51778370 For validation epoch 61, loss =0.51579243\n",
      "For training epoch 62, loss =0.51405078 For validation epoch 62, loss =0.51195663\n",
      "For training epoch 63, loss =0.51016682 For validation epoch 63, loss =0.50795716\n",
      "For training epoch 64, loss =0.50611728 For validation epoch 64, loss =0.50377160\n",
      "For training epoch 65, loss =0.50189233 For validation epoch 65, loss =0.49938899\n",
      "For training epoch 66, loss =0.49748707 For validation epoch 66, loss =0.49480551\n",
      "For training epoch 67, loss =0.49289101 For validation epoch 67, loss =0.49003032\n",
      "For training epoch 68, loss =0.48810714 For validation epoch 68, loss =0.48506522\n",
      "For training epoch 69, loss =0.48314857 For validation epoch 69, loss =0.47995391\n",
      "For training epoch 70, loss =0.47804067 For validation epoch 70, loss =0.47473130\n",
      "For training epoch 71, loss =0.47281459 For validation epoch 71, loss =0.46943384\n",
      "For training epoch 72, loss =0.46750876 For validation epoch 72, loss =0.46409744\n",
      "For training epoch 73, loss =0.46216518 For validation epoch 73, loss =0.45876876\n",
      "For training epoch 74, loss =0.45681283 For validation epoch 74, loss =0.45346999\n",
      "For training epoch 75, loss =0.45147747 For validation epoch 75, loss =0.44820997\n",
      "For training epoch 76, loss =0.44617981 For validation epoch 76, loss =0.44303328\n",
      "For training epoch 77, loss =0.44093096 For validation epoch 77, loss =0.43793848\n",
      "For training epoch 78, loss =0.43575117 For validation epoch 78, loss =0.43293637\n",
      "For training epoch 79, loss =0.43065813 For validation epoch 79, loss =0.42805013\n",
      "For training epoch 80, loss =0.42566413 For validation epoch 80, loss =0.42327812\n",
      "For training epoch 81, loss =0.42077160 For validation epoch 81, loss =0.41861406\n",
      "For training epoch 82, loss =0.41598645 For validation epoch 82, loss =0.41406572\n",
      "For training epoch 83, loss =0.41131896 For validation epoch 83, loss =0.40967420\n",
      "For training epoch 84, loss =0.40677404 For validation epoch 84, loss =0.40541056\n",
      "For training epoch 85, loss =0.40234911 For validation epoch 85, loss =0.40124390\n",
      "For training epoch 86, loss =0.39804280 For validation epoch 86, loss =0.39723381\n",
      "For training epoch 87, loss =0.39386618 For validation epoch 87, loss =0.39337173\n",
      "For training epoch 88, loss =0.38983828 For validation epoch 88, loss =0.38963690\n",
      "For training epoch 89, loss =0.38595021 For validation epoch 89, loss =0.38606519\n",
      "For training epoch 90, loss =0.38217899 For validation epoch 90, loss =0.38260683\n",
      "For training epoch 91, loss =0.37852663 For validation epoch 91, loss =0.37923053\n",
      "For training epoch 92, loss =0.37499431 For validation epoch 92, loss =0.37600181\n",
      "For training epoch 93, loss =0.37156811 For validation epoch 93, loss =0.37283477\n",
      "For training epoch 94, loss =0.36824903 For validation epoch 94, loss =0.36977750\n",
      "For training epoch 95, loss =0.36503133 For validation epoch 95, loss =0.36683321\n",
      "For training epoch 96, loss =0.36190313 For validation epoch 96, loss =0.36392802\n",
      "For training epoch 97, loss =0.35886475 For validation epoch 97, loss =0.36113095\n",
      "For training epoch 98, loss =0.35590935 For validation epoch 98, loss =0.35842213\n",
      "For training epoch 99, loss =0.35303131 For validation epoch 99, loss =0.35574305\n",
      "For training epoch 100, loss =0.35022697 For validation epoch 100, loss =0.35318366\n",
      "For training epoch 101, loss =0.34749430 For validation epoch 101, loss =0.35062551\n",
      "For training epoch 102, loss =0.34482956 For validation epoch 102, loss =0.34817421\n",
      "For training epoch 103, loss =0.34223428 For validation epoch 103, loss =0.34581578\n",
      "For training epoch 104, loss =0.33970046 For validation epoch 104, loss =0.34347004\n",
      "For training epoch 105, loss =0.33722034 For validation epoch 105, loss =0.34126785\n",
      "For training epoch 106, loss =0.33479625 For validation epoch 106, loss =0.33903468\n",
      "For training epoch 107, loss =0.33243346 For validation epoch 107, loss =0.33690536\n",
      "For training epoch 108, loss =0.33011705 For validation epoch 108, loss =0.33478338\n",
      "For training epoch 109, loss =0.32784179 For validation epoch 109, loss =0.33271652\n",
      "For training epoch 110, loss =0.32561588 For validation epoch 110, loss =0.33072564\n",
      "For training epoch 111, loss =0.32342806 For validation epoch 111, loss =0.32872465\n",
      "For training epoch 112, loss =0.32128584 For validation epoch 112, loss =0.32680690\n",
      "For training epoch 113, loss =0.31917730 For validation epoch 113, loss =0.32485563\n",
      "For training epoch 114, loss =0.31709984 For validation epoch 114, loss =0.32299361\n",
      "For training epoch 115, loss =0.31506199 For validation epoch 115, loss =0.32111782\n",
      "For training epoch 116, loss =0.31305480 For validation epoch 116, loss =0.31930113\n",
      "For training epoch 117, loss =0.31107959 For validation epoch 117, loss =0.31753021\n",
      "For training epoch 118, loss =0.30913714 For validation epoch 118, loss =0.31577712\n",
      "For training epoch 119, loss =0.30722582 For validation epoch 119, loss =0.31410500\n",
      "For training epoch 120, loss =0.30534393 For validation epoch 120, loss =0.31237307\n",
      "For training epoch 121, loss =0.30349690 For validation epoch 121, loss =0.31083941\n",
      "For training epoch 122, loss =0.30168501 For validation epoch 122, loss =0.30904245\n",
      "For training epoch 123, loss =0.29989526 For validation epoch 123, loss =0.30763924\n",
      "For training epoch 124, loss =0.29814410 For validation epoch 124, loss =0.30578938\n",
      "For training epoch 125, loss =0.29638806 For validation epoch 125, loss =0.30432793\n",
      "For training epoch 126, loss =0.29464233 For validation epoch 126, loss =0.30269396\n",
      "For training epoch 127, loss =0.29293254 For validation epoch 127, loss =0.30109993\n",
      "For training epoch 128, loss =0.29127124 For validation epoch 128, loss =0.29975981\n",
      "For training epoch 129, loss =0.28962868 For validation epoch 129, loss =0.29810488\n",
      "For training epoch 130, loss =0.28798360 For validation epoch 130, loss =0.29667285\n",
      "For training epoch 131, loss =0.28636160 For validation epoch 131, loss =0.29525951\n",
      "For training epoch 132, loss =0.28477293 For validation epoch 132, loss =0.29373607\n",
      "For training epoch 133, loss =0.28320581 For validation epoch 133, loss =0.29245692\n",
      "For training epoch 134, loss =0.28164744 For validation epoch 134, loss =0.29095879\n",
      "For training epoch 135, loss =0.28010139 For validation epoch 135, loss =0.28960052\n",
      "For training epoch 136, loss =0.27858010 For validation epoch 136, loss =0.28830877\n",
      "For training epoch 137, loss =0.27708381 For validation epoch 137, loss =0.28687871\n",
      "For training epoch 138, loss =0.27560487 For validation epoch 138, loss =0.28562531\n",
      "For training epoch 139, loss =0.27413797 For validation epoch 139, loss =0.28426284\n",
      "For training epoch 140, loss =0.27267712 For validation epoch 140, loss =0.28299409\n",
      "For training epoch 141, loss =0.27124104 For validation epoch 141, loss =0.28174359\n",
      "For training epoch 142, loss =0.26982427 For validation epoch 142, loss =0.28038564\n",
      "For training epoch 143, loss =0.26843244 For validation epoch 143, loss =0.27922267\n",
      "For training epoch 144, loss =0.26703891 For validation epoch 144, loss =0.27792880\n",
      "For training epoch 145, loss =0.26566342 For validation epoch 145, loss =0.27669811\n",
      "For training epoch 146, loss =0.26429906 For validation epoch 146, loss =0.27544367\n",
      "For training epoch 147, loss =0.26295397 For validation epoch 147, loss =0.27425274\n",
      "For training epoch 148, loss =0.26162192 For validation epoch 148, loss =0.27304065\n",
      "For training epoch 149, loss =0.26030144 For validation epoch 149, loss =0.27177864\n",
      "For training epoch 150, loss =0.25899956 For validation epoch 150, loss =0.27065951\n",
      "For training epoch 151, loss =0.25770625 For validation epoch 151, loss =0.26946524\n",
      "For training epoch 152, loss =0.25643155 For validation epoch 152, loss =0.26838881\n",
      "For training epoch 153, loss =0.25517499 For validation epoch 153, loss =0.26715648\n",
      "For training epoch 154, loss =0.25394791 For validation epoch 154, loss =0.26627052\n",
      "For training epoch 155, loss =0.25276265 For validation epoch 155, loss =0.26497659\n",
      "For training epoch 156, loss =0.25161061 For validation epoch 156, loss =0.26422727\n",
      "For training epoch 157, loss =0.25046819 For validation epoch 157, loss =0.26282629\n",
      "For training epoch 158, loss =0.24919862 For validation epoch 158, loss =0.26186785\n",
      "For training epoch 159, loss =0.24798281 For validation epoch 159, loss =0.26099122\n",
      "For training epoch 160, loss =0.24689665 For validation epoch 160, loss =0.25979757\n",
      "For training epoch 161, loss =0.24580929 For validation epoch 161, loss =0.25894904\n",
      "For training epoch 162, loss =0.24464834 For validation epoch 162, loss =0.25787836\n",
      "For training epoch 163, loss =0.24349457 For validation epoch 163, loss =0.25687519\n",
      "For training epoch 164, loss =0.24245325 For validation epoch 164, loss =0.25613576\n",
      "For training epoch 165, loss =0.24140295 For validation epoch 165, loss =0.25497255\n",
      "For training epoch 166, loss =0.24028002 For validation epoch 166, loss =0.25409761\n",
      "For training epoch 167, loss =0.23919831 For validation epoch 167, loss =0.25333592\n",
      "For training epoch 168, loss =0.23819292 For validation epoch 168, loss =0.25221729\n",
      "For training epoch 169, loss =0.23715492 For validation epoch 169, loss =0.25138831\n",
      "For training epoch 170, loss =0.23608854 For validation epoch 170, loss =0.25048646\n",
      "For training epoch 171, loss =0.23505518 For validation epoch 171, loss =0.24952087\n",
      "For training epoch 172, loss =0.23407367 For validation epoch 172, loss =0.24876292\n",
      "For training epoch 173, loss =0.23309208 For validation epoch 173, loss =0.24767072\n",
      "For training epoch 174, loss =0.23205282 For validation epoch 174, loss =0.24685945\n",
      "For training epoch 175, loss =0.23103248 For validation epoch 175, loss =0.24603081\n",
      "For training epoch 176, loss =0.23005585 For validation epoch 176, loss =0.24497697\n",
      "For training epoch 177, loss =0.22910142 For validation epoch 177, loss =0.24429892\n",
      "For training epoch 178, loss =0.22810061 For validation epoch 178, loss =0.24334694\n",
      "For training epoch 179, loss =0.22709855 For validation epoch 179, loss =0.24245809\n",
      "For training epoch 180, loss =0.22612397 For validation epoch 180, loss =0.24171531\n",
      "For training epoch 181, loss =0.22518855 For validation epoch 181, loss =0.24071072\n",
      "For training epoch 182, loss =0.22422056 For validation epoch 182, loss =0.23994225\n",
      "For training epoch 183, loss =0.22325145 For validation epoch 183, loss =0.23897396\n",
      "For training epoch 184, loss =0.22229242 For validation epoch 184, loss =0.23803806\n",
      "For training epoch 185, loss =0.22136879 For validation epoch 185, loss =0.23732704\n",
      "For training epoch 186, loss =0.22044455 For validation epoch 186, loss =0.23638707\n",
      "For training epoch 187, loss =0.21950899 For validation epoch 187, loss =0.23562907\n",
      "For training epoch 188, loss =0.21857321 For validation epoch 188, loss =0.23480500\n",
      "For training epoch 189, loss =0.21765041 For validation epoch 189, loss =0.23402309\n",
      "For training epoch 190, loss =0.21674344 For validation epoch 190, loss =0.23334183\n",
      "For training epoch 191, loss =0.21584465 For validation epoch 191, loss =0.23243159\n",
      "For training epoch 192, loss =0.21495251 For validation epoch 192, loss =0.23178284\n",
      "For training epoch 193, loss =0.21405686 For validation epoch 193, loss =0.23086601\n",
      "For training epoch 194, loss =0.21316184 For validation epoch 194, loss =0.23015256\n",
      "For training epoch 195, loss =0.21226811 For validation epoch 195, loss =0.22931670\n",
      "For training epoch 196, loss =0.21138813 For validation epoch 196, loss =0.22862445\n",
      "For training epoch 197, loss =0.21051297 For validation epoch 197, loss =0.22786495\n",
      "For training epoch 198, loss =0.20964175 For validation epoch 198, loss =0.22709440\n",
      "For training epoch 199, loss =0.20877877 For validation epoch 199, loss =0.22642042\n",
      "For training epoch 200, loss =0.20792238 For validation epoch 200, loss =0.22563982\n",
      "For training epoch 201, loss =0.20707825 For validation epoch 201, loss =0.22495417\n",
      "For training epoch 202, loss =0.20623967 For validation epoch 202, loss =0.22410715\n",
      "For training epoch 203, loss =0.20541120 For validation epoch 203, loss =0.22356282\n",
      "For training epoch 204, loss =0.20459564 For validation epoch 204, loss =0.22266006\n",
      "For training epoch 205, loss =0.20379497 For validation epoch 205, loss =0.22227171\n",
      "For training epoch 206, loss =0.20302552 For validation epoch 206, loss =0.22131771\n",
      "For training epoch 207, loss =0.20226356 For validation epoch 207, loss =0.22112671\n",
      "For training epoch 208, loss =0.20152263 For validation epoch 208, loss =0.21998940\n",
      "For training epoch 209, loss =0.20069830 For validation epoch 209, loss =0.21961018\n",
      "For training epoch 210, loss =0.19984305 For validation epoch 210, loss =0.21878637\n",
      "For training epoch 211, loss =0.19902018 For validation epoch 211, loss =0.21811394\n",
      "For training epoch 212, loss =0.19828503 For validation epoch 212, loss =0.21775000\n",
      "For training epoch 213, loss =0.19759740 For validation epoch 213, loss =0.21684107\n",
      "For training epoch 214, loss =0.19685961 For validation epoch 214, loss =0.21649331\n",
      "For training epoch 215, loss =0.19608238 For validation epoch 215, loss =0.21565145\n",
      "For training epoch 216, loss =0.19529396 For validation epoch 216, loss =0.21506713\n",
      "For training epoch 217, loss =0.19455646 For validation epoch 217, loss =0.21464241\n",
      "For training epoch 218, loss =0.19385830 For validation epoch 218, loss =0.21386816\n",
      "For training epoch 219, loss =0.19316503 For validation epoch 219, loss =0.21355312\n",
      "For training epoch 220, loss =0.19246952 For validation epoch 220, loss =0.21268462\n",
      "For training epoch 221, loss =0.19173186 For validation epoch 221, loss =0.21224126\n",
      "For training epoch 222, loss =0.19099736 For validation epoch 222, loss =0.21155274\n",
      "For training epoch 223, loss =0.19028212 For validation epoch 223, loss =0.21093273\n",
      "For training epoch 224, loss =0.18960163 For validation epoch 224, loss =0.21057288\n",
      "For training epoch 225, loss =0.18893594 For validation epoch 225, loss =0.20984189\n",
      "For training epoch 226, loss =0.18827298 For validation epoch 226, loss =0.20953153\n",
      "For training epoch 227, loss =0.18760212 For validation epoch 227, loss =0.20872355\n",
      "For training epoch 228, loss =0.18690433 For validation epoch 228, loss =0.20837057\n",
      "For training epoch 229, loss =0.18621348 For validation epoch 229, loss =0.20770437\n",
      "For training epoch 230, loss =0.18553177 For validation epoch 230, loss =0.20718962\n",
      "For training epoch 231, loss =0.18486457 For validation epoch 231, loss =0.20675224\n",
      "For training epoch 232, loss =0.18420522 For validation epoch 232, loss =0.20614725\n",
      "For training epoch 233, loss =0.18355788 For validation epoch 233, loss =0.20581716\n",
      "For training epoch 234, loss =0.18292356 For validation epoch 234, loss =0.20511186\n",
      "For training epoch 235, loss =0.18229686 For validation epoch 235, loss =0.20500007\n",
      "For training epoch 236, loss =0.18169431 For validation epoch 236, loss =0.20413454\n",
      "For training epoch 237, loss =0.18110088 For validation epoch 237, loss =0.20414746\n",
      "For training epoch 238, loss =0.18051307 For validation epoch 238, loss =0.20308912\n",
      "For training epoch 239, loss =0.17985323 For validation epoch 239, loss =0.20292474\n",
      "For training epoch 240, loss =0.17916265 For validation epoch 240, loss =0.20212168\n",
      "For training epoch 241, loss =0.17847146 For validation epoch 241, loss =0.20169912\n",
      "For training epoch 242, loss =0.17785196 For validation epoch 242, loss =0.20151794\n",
      "For training epoch 243, loss =0.17729454 For validation epoch 243, loss =0.20075719\n",
      "For training epoch 244, loss =0.17673706 For validation epoch 244, loss =0.20070860\n",
      "For training epoch 245, loss =0.17614691 For validation epoch 245, loss =0.19982305\n",
      "For training epoch 246, loss =0.17550458 For validation epoch 246, loss =0.19954737\n",
      "For training epoch 247, loss =0.17486355 For validation epoch 247, loss =0.19905263\n",
      "For training epoch 248, loss =0.17426121 For validation epoch 248, loss =0.19855066\n",
      "For training epoch 249, loss =0.17370422 For validation epoch 249, loss =0.19843376\n",
      "For training epoch 250, loss =0.17316911 For validation epoch 250, loss =0.19762266\n",
      "For training epoch 251, loss =0.17261088 For validation epoch 251, loss =0.19752403\n",
      "For training epoch 252, loss =0.17201604 For validation epoch 252, loss =0.19678058\n",
      "For training epoch 253, loss =0.17139916 For validation epoch 253, loss =0.19645357\n",
      "For training epoch 254, loss =0.17080757 For validation epoch 254, loss =0.19608201\n",
      "For training epoch 255, loss =0.17025185 For validation epoch 255, loss =0.19549870\n",
      "For training epoch 256, loss =0.16972004 For validation epoch 256, loss =0.19537944\n",
      "For training epoch 257, loss =0.16919897 For validation epoch 257, loss =0.19458845\n",
      "For training epoch 258, loss =0.16865866 For validation epoch 258, loss =0.19452333\n",
      "For training epoch 259, loss =0.16810061 For validation epoch 259, loss =0.19372813\n",
      "For training epoch 260, loss =0.16751169 For validation epoch 260, loss =0.19348150\n",
      "For training epoch 261, loss =0.16693479 For validation epoch 261, loss =0.19300222\n",
      "For training epoch 262, loss =0.16638133 For validation epoch 262, loss =0.19252498\n",
      "For training epoch 263, loss =0.16585179 For validation epoch 263, loss =0.19229092\n",
      "For training epoch 264, loss =0.16533428 For validation epoch 264, loss =0.19167702\n",
      "For training epoch 265, loss =0.16481854 For validation epoch 265, loss =0.19162042\n",
      "For training epoch 266, loss =0.16430108 For validation epoch 266, loss =0.19095734\n",
      "For training epoch 267, loss =0.16376531 For validation epoch 267, loss =0.19081590\n",
      "For training epoch 268, loss =0.16322511 For validation epoch 268, loss =0.19023976\n",
      "For training epoch 269, loss =0.16268544 For validation epoch 269, loss =0.18995632\n",
      "For training epoch 270, loss =0.16215655 For validation epoch 270, loss =0.18958887\n",
      "For training epoch 271, loss =0.16163971 For validation epoch 271, loss =0.18915644\n",
      "For training epoch 272, loss =0.16112950 For validation epoch 272, loss =0.18889001\n",
      "For training epoch 273, loss =0.16062774 For validation epoch 273, loss =0.18833785\n",
      "For training epoch 274, loss =0.16013424 For validation epoch 274, loss =0.18822911\n",
      "For training epoch 275, loss =0.15964572 For validation epoch 275, loss =0.18764216\n",
      "For training epoch 276, loss =0.15915744 For validation epoch 276, loss =0.18762800\n",
      "For training epoch 277, loss =0.15867451 For validation epoch 277, loss =0.18696864\n",
      "For training epoch 278, loss =0.15818395 For validation epoch 278, loss =0.18705511\n",
      "For training epoch 279, loss =0.15770876 For validation epoch 279, loss =0.18637744\n",
      "For training epoch 280, loss =0.15721975 For validation epoch 280, loss =0.18638612\n",
      "For training epoch 281, loss =0.15673962 For validation epoch 281, loss =0.18563595\n",
      "For training epoch 282, loss =0.15623918 For validation epoch 282, loss =0.18559164\n",
      "For training epoch 283, loss =0.15574585 For validation epoch 283, loss =0.18493128\n",
      "For training epoch 284, loss =0.15525405 For validation epoch 284, loss =0.18478377\n",
      "For training epoch 285, loss =0.15476359 For validation epoch 285, loss =0.18426508\n",
      "For training epoch 286, loss =0.15427788 For validation epoch 286, loss =0.18411215\n",
      "For training epoch 287, loss =0.15379930 For validation epoch 287, loss =0.18385078\n",
      "For training epoch 288, loss =0.15333408 For validation epoch 288, loss =0.18348187\n",
      "For training epoch 289, loss =0.15287533 For validation epoch 289, loss =0.18326896\n",
      "For training epoch 290, loss =0.15242299 For validation epoch 290, loss =0.18287997\n",
      "For training epoch 291, loss =0.15197785 For validation epoch 291, loss =0.18301868\n",
      "For training epoch 292, loss =0.15155464 For validation epoch 292, loss =0.18234058\n",
      "For training epoch 293, loss =0.15113868 For validation epoch 293, loss =0.18246621\n",
      "For training epoch 294, loss =0.15075567 For validation epoch 294, loss =0.18170081\n",
      "For training epoch 295, loss =0.15036076 For validation epoch 295, loss =0.18221450\n",
      "For training epoch 296, loss =0.14998055 For validation epoch 296, loss =0.18130179\n",
      "For training epoch 297, loss =0.14949477 For validation epoch 297, loss =0.18139069\n",
      "For training epoch 298, loss =0.14897315 For validation epoch 298, loss =0.18071643\n",
      "For training epoch 299, loss =0.14845435 For validation epoch 299, loss =0.18057145\n",
      "For training epoch 300, loss =0.14801507 For validation epoch 300, loss =0.18058738\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 500\n",
    "learning_rate = 0.001\n",
    "\n",
    "input_size = 48\n",
    "hidden_size = 32\n",
    "num_layers = 1\n",
    "num_classes = 1 \n",
    "\n",
    "\n",
    "model = LSTM1(num_classes, input_size, hidden_size, num_layers, x_train.shape[1])\n",
    "\n",
    "# Loss and optimizer\n",
    "weight = torch.Tensor([5])\n",
    "loss = nn.BCELoss(weight=weight)#pos_weight=torch.tensor([2.0]))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "t_acc = []\n",
    "v_acc = []\n",
    "t_correct = 0\n",
    "v_correct = 0\n",
    "\n",
    "#Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    outputs = model.forward(x_train)\n",
    "    optimizer.zero_grad()\n",
    "    l = loss(outputs, y_train)\n",
    "    train_loss.append(l.item())\n",
    "    l.backward()\n",
    "    optimizer.step() \n",
    "\n",
    "    #accuracy\n",
    "    t_correct += outputs.round().eq(y_train).sum().item()\n",
    "    t_acc.append(t_correct/y_train.shape[0])\n",
    "\n",
    "     #calculate and plot the validation loss\n",
    "    model.eval()\n",
    "    y_pred_test = model(x_valid)\n",
    "    lv = loss(y_pred_test, y_valid)\n",
    "    #append the loss per batch\n",
    "    valid_loss.append(lv.item())\n",
    "\n",
    "    #accuracy\n",
    "    v_correct += y_pred_test.round().eq(y_valid).sum().item()\n",
    "    v_acc.append(v_correct/y_valid.shape[0])\n",
    "        \n",
    "   \n",
    "    #append the total loss and accuracy per epoch\n",
    "\n",
    "    print(f'For training epoch {epoch+1}, loss ={l:.8f}', f'For validation epoch {epoch+1}, loss ={lv:.8f}'  )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for slip detection is 0.9855705720963317\n",
      "Fscore for slip detection is 0.9467536693126192\n",
      "Precision for slip detection is 0.9557941267711845\n",
      "Recall for slip detection is 0.9381309218133471\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x16bc1bb50>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#validation \n",
    "y_predict = model.forward(x_valid)\n",
    "y_predict = y_predict.round()\n",
    "y_predict = y_predict.data.numpy()\n",
    "y_valid = y_valid\n",
    "lstm_metrics(y_predict, y_valid)\n",
    "\n",
    "#print the loss\n",
    "plt.plot(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7.7758e-05],\n",
       "        [2.4002e-06],\n",
       "        [1.0070e-08],\n",
       "        ...,\n",
       "        [3.4911e-06],\n",
       "        [1.5728e-05],\n",
       "        [9.9251e-01]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(x_valid)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Increase in the no of epochs increased Model's accuracy and generalization effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# torch.save(model.state_dict(), \"saved_models/LSTM_working2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To load model\n",
    "\n",
    "# newmodel = LSTM1(num_classes, input_size, hidden_size, num_layers, x_train.shape[1])\n",
    "# newmodel.load_state_dict(torch.load(\"saved_models/LSTM_working.pth\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nei",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "173c0cb41f479ae2d1f90bf66f9ae3aceca0c8feada6413b4ebace4131a19a6b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
