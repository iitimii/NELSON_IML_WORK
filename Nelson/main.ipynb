{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn.functional as F  # Parameterless functions, like (some) activation functions\n",
    "import torchvision.datasets as datasets  # Standard datasets\n",
    "import torchvision.transforms as transforms  # Transformations we can perform on our dataset for augmentation\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset  # Gives easier dataset managment by creating mini batches etc.\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, f1_score, recall_score, precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_xela_allfiles = pd.read_csv('data.csv', index_col=0)\n",
    "pd_sliplabel_allfiles = pd.read_csv('labels.csv')\n",
    "pd_sliplabel_allfiles = pd_sliplabel_allfiles['slip']\n",
    "tac_label = pd_sliplabel_allfiles.values.reshape(pd_sliplabel_allfiles.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_x = []\n",
    "col_y = []\n",
    "col_z = []\n",
    "\n",
    "pd_columns = pd_xela_allfiles.columns\n",
    "for col in pd_columns:\n",
    "    if col.endswith('x'):\n",
    "        col_x.append(col)\n",
    "    \n",
    "    elif col.endswith('y'):\n",
    "        col_y.append(col)\n",
    "    \n",
    "    elif col.endswith('z'):\n",
    "        col_z.append(col)\n",
    "\n",
    "pd_xela_allfiles_x = pd_xela_allfiles[col_x]\n",
    "pd_xela_allfiles_y = pd_xela_allfiles[col_y]\n",
    "pd_xela_allfiles_z = pd_xela_allfiles[col_z]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = MinMaxScaler()\n",
    "sc.fit(pd_xela_allfiles_x)\n",
    "pd_xela_allfiles_x = sc.transform(pd_xela_allfiles_x)\n",
    "\n",
    "sc.fit(pd_xela_allfiles_y)\n",
    "pd_xela_allfiles_y = sc.transform(pd_xela_allfiles_y)\n",
    "\n",
    "sc.fit(pd_xela_allfiles_z)\n",
    "pd_xela_allfiles_z = sc.transform(pd_xela_allfiles_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_x = []\n",
    "pd_y = []\n",
    "pd_z = []\n",
    "\n",
    "for row in range(len(pd_xela_allfiles_x)):\n",
    "    pd_x.append(np.rot90(pd_xela_allfiles_x[row].reshape(4,4)))\n",
    "    pd_y.append(np.rot90(pd_xela_allfiles_y[row].reshape(4,4)))\n",
    "    pd_z.append(np.rot90(pd_xela_allfiles_z[row].reshape(4,4)))\n",
    "\n",
    "pd_main = [pd_x, pd_y, pd_z]\n",
    "pd_image = np.zeros( (pd_xela_allfiles.shape[0], 3, 4, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#per row, get (4,4) of x, y, z and assign it to pd_image to form the image\n",
    "for row in range(pd_xela_allfiles.shape[0]):\n",
    "    x_4_4 = pd_main[0][row]\n",
    "    y_4_4 = pd_main[1][row]\n",
    "    z_4_4 = pd_main[2][row]\n",
    "\n",
    "    pd_image[row][0] = x_4_4\n",
    "    pd_image[row][1] = y_4_4\n",
    "    pd_image[row][2] = z_4_4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# up_size = 4\n",
    "# n_images = len(pd_x)\n",
    "\n",
    "# tac_imagex = np.zeros((n_images, up_size, up_size), np.float32)\n",
    "# tac_imagey = np.zeros((n_images, up_size, up_size), np.float32) \n",
    "# tac_imagez = np.zeros((n_images, up_size, up_size), np.float32) \n",
    "# #resize image to 3, up_size, up_size\n",
    "\n",
    "# for row in range(len(pd_x)):\n",
    "#     img_4_4_x = pd_x[row]\n",
    "#     tac_imagex[row] = cv2.resize(img_4_4_x.astype(np.float32), dsize=(up_size, up_size), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "# for row in range(len(pd_y)):\n",
    "#     img_4_4_y = pd_y[row]\n",
    "#     tac_imagey[row] = cv2.resize(img_4_4_y.astype(np.float32), dsize=(up_size, up_size), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "# for row in range(len(pd_z)):\n",
    "#     img_4_4_z = pd_z[row]\n",
    "#     tac_imagez[row] = cv2.resize(img_4_4_z.astype(np.float32), dsize=(up_size, up_size), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "# pd_main = [tac_imagex, tac_imagey, tac_imagez]\n",
    "\n",
    "# #arrange pd_main in a 3, 224, 224 array where its 3(224, 224) of x, y, z values\n",
    "# pd_image = np.zeros( (pd_xela_allfiles.shape[0], 3, up_size, up_size))\n",
    "\n",
    "# #per row, get (224,224) of x, y, z and assign it to pd_image to form the image\n",
    "# for row in range(pd_xela_allfiles.shape[0]):\n",
    "#     x_4_4 = pd_main[0][row]\n",
    "#     y_4_4 = pd_main[1][row]\n",
    "#     z_4_4 = pd_main[2][row]\n",
    "\n",
    "#     pd_image[row][0] = x_4_4\n",
    "#     pd_image[row][1] = y_4_4\n",
    "#     pd_image[row][2] = z_4_4\n",
    "\n",
    "# plt.imshow(tac_imagex[100040])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# up_size = 16\n",
    "# n_images = len(pd_x)\n",
    "# tac_image = np.zeros((n_images, 3, up_size, up_size), np.float32) \n",
    "# for row in range(n_images):\n",
    "\n",
    "#     #resize image to 3, up_size, up_size\n",
    "#     for channel in range(3):\n",
    "#         image_per_channel = pd_image[row][channel]\n",
    "#         tac_image[row][channel] = cv2.resize(image_per_channel.astype(np.float32), dsize=(up_size, up_size), interpolation=cv2.INTER_CUBIC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tac_image = torch.from_numpy(pd_image.astype(np.float32))\n",
    "tac_label = torch.from_numpy(tac_label.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into train and test\n",
    "tac_image_train, tac_image_test, tac_label_train, tac_label_test = train_test_split(tac_image, tac_label, test_size=0.1, shuffle=True)\n",
    "\n",
    "#split into train and validation\n",
    "tac_image_train, tac_image_valid, tac_label_train, tac_label_valid = train_test_split(tac_image_train, tac_label_train, test_size=0.3, shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch_Taxels(Dataset):\n",
    "    \n",
    "    def __init__(self, tac_image_train, tac_label_train, tac_image_valid, tac_label_valid, valid = None):\n",
    "        self.x = tac_image_train\n",
    "        self.y = tac_label_train\n",
    "        self.xvalid = tac_image_valid\n",
    "        self.yvalid = tac_label_valid\n",
    "        self.valid = valid\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.valid == True:\n",
    "            return self.xvalid.shape[0]\n",
    "        else:\n",
    "            return self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if self.valid == True:\n",
    "            return self.xvalid[idx], self.yvalid[idx]\n",
    "        else:\n",
    "            return self.x[idx], self.y[idx]\n",
    "\n",
    "#Divide the data into batches\n",
    "dataset = Batch_Taxels(tac_image_train, tac_label_train, tac_image_valid, tac_label_valid)\n",
    "dataset2 = Batch_Taxels(tac_image_train, tac_label_train, tac_image_valid, tac_label_valid, valid = True)\n",
    "\n",
    "xelaloader = DataLoader(dataset = dataset, batch_size=32, shuffle=True)\n",
    "xelaloadervalid = DataLoader(dataset = dataset2, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 4, 4]) torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "for x, y in xelaloader:\n",
    "    print (x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputsize = (3,4,4)\n",
    "seq_length = 3 \n",
    "num_layers = 2 \n",
    "hidden_size = 2\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, inputsize, hidden_size, num_layers, num_classes = 1):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(inputsize, hidden_size, num_layers, batch_first = True)\n",
    "        self.fc  = nn.Linear(hidden_size*seq_length, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(num_layers, x.shape[0], inputsize)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "empty(): argument 'size' must be tuple of SymInts, but found element of type tuple at pos 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [62], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m RNN(inputsize, hidden_size, num_layers, num_classes \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m)\n\u001b[0;32m      2\u001b[0m loss \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mBCELoss()\n\u001b[0;32m      3\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mlearning_rate)\n",
      "Cell \u001b[1;32mIn [61], line 14\u001b[0m, in \u001b[0;36mRNN.__init__\u001b[1;34m(self, inputsize, hidden_size, num_layers, num_classes)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_size \u001b[39m=\u001b[39m hidden_size\n\u001b[0;32m     13\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers \u001b[39m=\u001b[39m num_layers\n\u001b[1;32m---> 14\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrnn \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mRNN(inputsize, hidden_size, num_layers, batch_first \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     15\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc  \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(hidden_size\u001b[39m*\u001b[39mseq_length, num_classes)\n",
      "File \u001b[1;32mc:\\Users\\Timii\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:425\u001b[0m, in \u001b[0;36mRNN.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    423\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    424\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mUnknown nonlinearity \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnonlinearity))\n\u001b[1;32m--> 425\u001b[0m \u001b[39msuper\u001b[39m(RNN, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(mode, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Timii\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:94\u001b[0m, in \u001b[0;36mRNNBase.__init__\u001b[1;34m(self, mode, input_size, hidden_size, num_layers, bias, batch_first, dropout, bidirectional, proj_size, device, dtype)\u001b[0m\n\u001b[0;32m     91\u001b[0m real_hidden_size \u001b[39m=\u001b[39m proj_size \u001b[39mif\u001b[39;00m proj_size \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m hidden_size\n\u001b[0;32m     92\u001b[0m layer_input_size \u001b[39m=\u001b[39m input_size \u001b[39mif\u001b[39;00m layer \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m real_hidden_size \u001b[39m*\u001b[39m num_directions\n\u001b[1;32m---> 94\u001b[0m w_ih \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39mempty((gate_size, layer_input_size), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfactory_kwargs))\n\u001b[0;32m     95\u001b[0m w_hh \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39mempty((gate_size, real_hidden_size), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfactory_kwargs))\n\u001b[0;32m     96\u001b[0m b_ih \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39mempty(gate_size, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfactory_kwargs))\n",
      "\u001b[1;31mTypeError\u001b[0m: empty(): argument 'size' must be tuple of SymInts, but found element of type tuple at pos 2"
     ]
    }
   ],
   "source": [
    "model = RNN(inputsize, hidden_size, num_layers, num_classes = 1)\n",
    "loss = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "\n",
    "t_loss = []\n",
    "v_loss = []\n",
    "\n",
    "t_acc = []\n",
    "v_acc = []\n",
    "\n",
    "t_acc_t = []\n",
    "v_acc_t = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "zeros(): argument 'size' must be tuple of SymInts, but found element of type tuple at pos 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [63], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m (x, y) \u001b[39min\u001b[39;00m (xelaloader):\n\u001b[0;32m      8\u001b[0m     \u001b[39m#Forward pass\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m     y_pred \u001b[39m=\u001b[39m model(x)\n\u001b[0;32m     11\u001b[0m     \u001b[39m#compute the loss\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     l \u001b[39m=\u001b[39m loss(y_pred, y)\n",
      "File \u001b[1;32mc:\\Users\\Timii\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn [58], line 18\u001b[0m, in \u001b[0;36mRNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> 18\u001b[0m     h0 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mzeros( x\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m], inputsize)\n\u001b[0;32m     19\u001b[0m     out, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrnn(x, h0)\n\u001b[0;32m     20\u001b[0m     out \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mreshape(out\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: zeros(): argument 'size' must be tuple of SymInts, but found element of type tuple at pos 2"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    l_xelaloader = 0\n",
    "\n",
    "    model.train()\n",
    "    for (x, y) in (xelaloader):\n",
    "        #Forward pass\n",
    "        y_pred = model(x)\n",
    "       \n",
    "        #compute the loss\n",
    "        l = loss(y_pred, y)\n",
    "\n",
    "        #empty the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #compute the gradient\n",
    "        l.backward()\n",
    "\n",
    "        #update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        #append each loss per batch\n",
    "        train_loss.append(l.item())\n",
    "\n",
    "        #accuracy\n",
    "        total += y.size(0)\n",
    "        correct += y_pred.round().eq(y).sum().item()\n",
    "        l_xelaloader += x.shape[0]\n",
    "        \n",
    "    \n",
    "    t_acc = correct/l_xelaloader\n",
    "    t_acc_t.append(t_acc)\n",
    "\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    l_xelaloader = 0\n",
    "\n",
    "    #calculate and plot the validation loss\n",
    "    model.eval()\n",
    "    for (x,y) in (xelaloadervalid):\n",
    "        y_pred_test = model(x)\n",
    "        lv = loss(y_pred_test, y)\n",
    "        #append the loss per batch\n",
    "        valid_loss.append(lv.item())\n",
    "\n",
    "        #accuracy\n",
    "        total += y.size(0)\n",
    "        correct += y_pred_test.round().eq(y).sum().item()\n",
    "        l_xelaloader += x.shape[0]\n",
    "        \n",
    "    v_acc = correct/l_xelaloader\n",
    "    v_acc_t.append(v_acc)\n",
    "\n",
    "    #append the total loss and accuracy per epoch\n",
    "    t_loss.append(np.mean(train_loss))\n",
    "    v_loss.append(np.mean(valid_loss))\n",
    "\n",
    "    print(f'For training epoch {epoch+1}, loss ={l:.8f}', f'For validation epoch {epoch+1}, loss ={lv:.8f}'  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "947f030b3e678118fc438144c1e47ca5c23949e6feee86165ca58c1240ce2eba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
