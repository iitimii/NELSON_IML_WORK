{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', 500)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, f1_score, recall_score, precision_score\n",
    "\n",
    "from ViT_model import VisionTransformer\n",
    "from transformer_model import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.has_mps:\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(\"Device: {}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txl1_x</th>\n",
       "      <th>txl1_y</th>\n",
       "      <th>txl1_z</th>\n",
       "      <th>txl2_x</th>\n",
       "      <th>txl2_y</th>\n",
       "      <th>txl2_z</th>\n",
       "      <th>txl3_x</th>\n",
       "      <th>txl3_y</th>\n",
       "      <th>txl3_z</th>\n",
       "      <th>txl4_x</th>\n",
       "      <th>txl4_y</th>\n",
       "      <th>txl4_z</th>\n",
       "      <th>txl5_x</th>\n",
       "      <th>txl5_y</th>\n",
       "      <th>txl5_z</th>\n",
       "      <th>txl6_x</th>\n",
       "      <th>txl6_y</th>\n",
       "      <th>txl6_z</th>\n",
       "      <th>txl7_x</th>\n",
       "      <th>txl7_y</th>\n",
       "      <th>txl7_z</th>\n",
       "      <th>txl8_x</th>\n",
       "      <th>txl8_y</th>\n",
       "      <th>txl8_z</th>\n",
       "      <th>txl9_x</th>\n",
       "      <th>txl9_y</th>\n",
       "      <th>txl9_z</th>\n",
       "      <th>txl10_x</th>\n",
       "      <th>txl10_y</th>\n",
       "      <th>txl10_z</th>\n",
       "      <th>txl11_x</th>\n",
       "      <th>txl11_y</th>\n",
       "      <th>txl11_z</th>\n",
       "      <th>txl12_x</th>\n",
       "      <th>txl12_y</th>\n",
       "      <th>txl12_z</th>\n",
       "      <th>txl13_x</th>\n",
       "      <th>txl13_y</th>\n",
       "      <th>txl13_z</th>\n",
       "      <th>txl14_x</th>\n",
       "      <th>txl14_y</th>\n",
       "      <th>txl14_z</th>\n",
       "      <th>txl15_x</th>\n",
       "      <th>txl15_y</th>\n",
       "      <th>txl15_z</th>\n",
       "      <th>txl16_x</th>\n",
       "      <th>txl16_y</th>\n",
       "      <th>txl16_z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15825.0</td>\n",
       "      <td>16435.0</td>\n",
       "      <td>39825.0</td>\n",
       "      <td>15955.0</td>\n",
       "      <td>16459.0</td>\n",
       "      <td>39477.0</td>\n",
       "      <td>15767.0</td>\n",
       "      <td>16355.0</td>\n",
       "      <td>39049.0</td>\n",
       "      <td>15264.0</td>\n",
       "      <td>16591.0</td>\n",
       "      <td>40432.0</td>\n",
       "      <td>15804.0</td>\n",
       "      <td>16539.0</td>\n",
       "      <td>38817.0</td>\n",
       "      <td>15890.0</td>\n",
       "      <td>16367.0</td>\n",
       "      <td>38264.0</td>\n",
       "      <td>15758.0</td>\n",
       "      <td>16392.0</td>\n",
       "      <td>38276.0</td>\n",
       "      <td>15566.0</td>\n",
       "      <td>16494.0</td>\n",
       "      <td>38594.0</td>\n",
       "      <td>16179.0</td>\n",
       "      <td>16362.0</td>\n",
       "      <td>38096.0</td>\n",
       "      <td>15985.0</td>\n",
       "      <td>16319.0</td>\n",
       "      <td>38264.0</td>\n",
       "      <td>15794.0</td>\n",
       "      <td>16181.0</td>\n",
       "      <td>37601.0</td>\n",
       "      <td>15631.0</td>\n",
       "      <td>16283.0</td>\n",
       "      <td>38356.0</td>\n",
       "      <td>16171.0</td>\n",
       "      <td>16215.0</td>\n",
       "      <td>38774.0</td>\n",
       "      <td>15976.0</td>\n",
       "      <td>16356.0</td>\n",
       "      <td>38258.0</td>\n",
       "      <td>15887.0</td>\n",
       "      <td>16228.0</td>\n",
       "      <td>37885.0</td>\n",
       "      <td>15693.0</td>\n",
       "      <td>16427.0</td>\n",
       "      <td>38642.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15825.0</td>\n",
       "      <td>16435.0</td>\n",
       "      <td>39825.0</td>\n",
       "      <td>15955.0</td>\n",
       "      <td>16459.0</td>\n",
       "      <td>39477.0</td>\n",
       "      <td>15767.0</td>\n",
       "      <td>16355.0</td>\n",
       "      <td>39049.0</td>\n",
       "      <td>15264.0</td>\n",
       "      <td>16591.0</td>\n",
       "      <td>40432.0</td>\n",
       "      <td>15804.0</td>\n",
       "      <td>16539.0</td>\n",
       "      <td>38817.0</td>\n",
       "      <td>15890.0</td>\n",
       "      <td>16367.0</td>\n",
       "      <td>38264.0</td>\n",
       "      <td>15758.0</td>\n",
       "      <td>16392.0</td>\n",
       "      <td>38276.0</td>\n",
       "      <td>15566.0</td>\n",
       "      <td>16494.0</td>\n",
       "      <td>38594.0</td>\n",
       "      <td>16179.0</td>\n",
       "      <td>16362.0</td>\n",
       "      <td>38096.0</td>\n",
       "      <td>15985.0</td>\n",
       "      <td>16319.0</td>\n",
       "      <td>38264.0</td>\n",
       "      <td>15794.0</td>\n",
       "      <td>16181.0</td>\n",
       "      <td>37601.0</td>\n",
       "      <td>15631.0</td>\n",
       "      <td>16283.0</td>\n",
       "      <td>38356.0</td>\n",
       "      <td>16171.0</td>\n",
       "      <td>16215.0</td>\n",
       "      <td>38774.0</td>\n",
       "      <td>15976.0</td>\n",
       "      <td>16356.0</td>\n",
       "      <td>38258.0</td>\n",
       "      <td>15887.0</td>\n",
       "      <td>16228.0</td>\n",
       "      <td>37885.0</td>\n",
       "      <td>15693.0</td>\n",
       "      <td>16427.0</td>\n",
       "      <td>38642.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15826.0</td>\n",
       "      <td>16436.0</td>\n",
       "      <td>39818.0</td>\n",
       "      <td>15955.0</td>\n",
       "      <td>16459.0</td>\n",
       "      <td>39482.0</td>\n",
       "      <td>15767.0</td>\n",
       "      <td>16354.0</td>\n",
       "      <td>39049.0</td>\n",
       "      <td>15265.0</td>\n",
       "      <td>16589.0</td>\n",
       "      <td>40434.0</td>\n",
       "      <td>15805.0</td>\n",
       "      <td>16539.0</td>\n",
       "      <td>38812.0</td>\n",
       "      <td>15890.0</td>\n",
       "      <td>16365.0</td>\n",
       "      <td>38264.0</td>\n",
       "      <td>15756.0</td>\n",
       "      <td>16394.0</td>\n",
       "      <td>38277.0</td>\n",
       "      <td>15563.0</td>\n",
       "      <td>16493.0</td>\n",
       "      <td>38594.0</td>\n",
       "      <td>16179.0</td>\n",
       "      <td>16362.0</td>\n",
       "      <td>38092.0</td>\n",
       "      <td>15985.0</td>\n",
       "      <td>16320.0</td>\n",
       "      <td>38265.0</td>\n",
       "      <td>15793.0</td>\n",
       "      <td>16183.0</td>\n",
       "      <td>37597.0</td>\n",
       "      <td>15630.0</td>\n",
       "      <td>16283.0</td>\n",
       "      <td>38347.0</td>\n",
       "      <td>16172.0</td>\n",
       "      <td>16217.0</td>\n",
       "      <td>38777.0</td>\n",
       "      <td>15975.0</td>\n",
       "      <td>16356.0</td>\n",
       "      <td>38252.0</td>\n",
       "      <td>15888.0</td>\n",
       "      <td>16231.0</td>\n",
       "      <td>37885.0</td>\n",
       "      <td>15692.0</td>\n",
       "      <td>16429.0</td>\n",
       "      <td>38641.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15826.0</td>\n",
       "      <td>16435.0</td>\n",
       "      <td>39826.0</td>\n",
       "      <td>15956.0</td>\n",
       "      <td>16458.0</td>\n",
       "      <td>39478.0</td>\n",
       "      <td>15766.0</td>\n",
       "      <td>16354.0</td>\n",
       "      <td>39051.0</td>\n",
       "      <td>15264.0</td>\n",
       "      <td>16590.0</td>\n",
       "      <td>40438.0</td>\n",
       "      <td>15805.0</td>\n",
       "      <td>16540.0</td>\n",
       "      <td>38812.0</td>\n",
       "      <td>15892.0</td>\n",
       "      <td>16367.0</td>\n",
       "      <td>38269.0</td>\n",
       "      <td>15756.0</td>\n",
       "      <td>16394.0</td>\n",
       "      <td>38277.0</td>\n",
       "      <td>15563.0</td>\n",
       "      <td>16493.0</td>\n",
       "      <td>38594.0</td>\n",
       "      <td>16177.0</td>\n",
       "      <td>16360.0</td>\n",
       "      <td>38099.0</td>\n",
       "      <td>15986.0</td>\n",
       "      <td>16321.0</td>\n",
       "      <td>38265.0</td>\n",
       "      <td>15793.0</td>\n",
       "      <td>16183.0</td>\n",
       "      <td>37599.0</td>\n",
       "      <td>15630.0</td>\n",
       "      <td>16282.0</td>\n",
       "      <td>38353.0</td>\n",
       "      <td>16170.0</td>\n",
       "      <td>16215.0</td>\n",
       "      <td>38776.0</td>\n",
       "      <td>15975.0</td>\n",
       "      <td>16355.0</td>\n",
       "      <td>38257.0</td>\n",
       "      <td>15888.0</td>\n",
       "      <td>16231.0</td>\n",
       "      <td>37885.0</td>\n",
       "      <td>15692.0</td>\n",
       "      <td>16429.0</td>\n",
       "      <td>38641.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15826.0</td>\n",
       "      <td>16432.0</td>\n",
       "      <td>39827.0</td>\n",
       "      <td>15956.0</td>\n",
       "      <td>16458.0</td>\n",
       "      <td>39483.0</td>\n",
       "      <td>15768.0</td>\n",
       "      <td>16355.0</td>\n",
       "      <td>39051.0</td>\n",
       "      <td>15265.0</td>\n",
       "      <td>16589.0</td>\n",
       "      <td>40437.0</td>\n",
       "      <td>15806.0</td>\n",
       "      <td>16537.0</td>\n",
       "      <td>38811.0</td>\n",
       "      <td>15892.0</td>\n",
       "      <td>16366.0</td>\n",
       "      <td>38263.0</td>\n",
       "      <td>15757.0</td>\n",
       "      <td>16393.0</td>\n",
       "      <td>38283.0</td>\n",
       "      <td>15563.0</td>\n",
       "      <td>16495.0</td>\n",
       "      <td>38593.0</td>\n",
       "      <td>16179.0</td>\n",
       "      <td>16360.0</td>\n",
       "      <td>38092.0</td>\n",
       "      <td>15988.0</td>\n",
       "      <td>16320.0</td>\n",
       "      <td>38271.0</td>\n",
       "      <td>15793.0</td>\n",
       "      <td>16182.0</td>\n",
       "      <td>37601.0</td>\n",
       "      <td>15630.0</td>\n",
       "      <td>16284.0</td>\n",
       "      <td>38350.0</td>\n",
       "      <td>16171.0</td>\n",
       "      <td>16212.0</td>\n",
       "      <td>38774.0</td>\n",
       "      <td>15974.0</td>\n",
       "      <td>16356.0</td>\n",
       "      <td>38257.0</td>\n",
       "      <td>15887.0</td>\n",
       "      <td>16228.0</td>\n",
       "      <td>37884.0</td>\n",
       "      <td>15693.0</td>\n",
       "      <td>16426.0</td>\n",
       "      <td>38641.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    txl1_x   txl1_y   txl1_z   txl2_x   txl2_y   txl2_z   txl3_x   txl3_y   \n",
       "0  15825.0  16435.0  39825.0  15955.0  16459.0  39477.0  15767.0  16355.0  \\\n",
       "1  15825.0  16435.0  39825.0  15955.0  16459.0  39477.0  15767.0  16355.0   \n",
       "2  15826.0  16436.0  39818.0  15955.0  16459.0  39482.0  15767.0  16354.0   \n",
       "3  15826.0  16435.0  39826.0  15956.0  16458.0  39478.0  15766.0  16354.0   \n",
       "4  15826.0  16432.0  39827.0  15956.0  16458.0  39483.0  15768.0  16355.0   \n",
       "\n",
       "    txl3_z   txl4_x   txl4_y   txl4_z   txl5_x   txl5_y   txl5_z   txl6_x   \n",
       "0  39049.0  15264.0  16591.0  40432.0  15804.0  16539.0  38817.0  15890.0  \\\n",
       "1  39049.0  15264.0  16591.0  40432.0  15804.0  16539.0  38817.0  15890.0   \n",
       "2  39049.0  15265.0  16589.0  40434.0  15805.0  16539.0  38812.0  15890.0   \n",
       "3  39051.0  15264.0  16590.0  40438.0  15805.0  16540.0  38812.0  15892.0   \n",
       "4  39051.0  15265.0  16589.0  40437.0  15806.0  16537.0  38811.0  15892.0   \n",
       "\n",
       "    txl6_y   txl6_z   txl7_x   txl7_y   txl7_z   txl8_x   txl8_y   txl8_z   \n",
       "0  16367.0  38264.0  15758.0  16392.0  38276.0  15566.0  16494.0  38594.0  \\\n",
       "1  16367.0  38264.0  15758.0  16392.0  38276.0  15566.0  16494.0  38594.0   \n",
       "2  16365.0  38264.0  15756.0  16394.0  38277.0  15563.0  16493.0  38594.0   \n",
       "3  16367.0  38269.0  15756.0  16394.0  38277.0  15563.0  16493.0  38594.0   \n",
       "4  16366.0  38263.0  15757.0  16393.0  38283.0  15563.0  16495.0  38593.0   \n",
       "\n",
       "    txl9_x   txl9_y   txl9_z  txl10_x  txl10_y  txl10_z  txl11_x  txl11_y   \n",
       "0  16179.0  16362.0  38096.0  15985.0  16319.0  38264.0  15794.0  16181.0  \\\n",
       "1  16179.0  16362.0  38096.0  15985.0  16319.0  38264.0  15794.0  16181.0   \n",
       "2  16179.0  16362.0  38092.0  15985.0  16320.0  38265.0  15793.0  16183.0   \n",
       "3  16177.0  16360.0  38099.0  15986.0  16321.0  38265.0  15793.0  16183.0   \n",
       "4  16179.0  16360.0  38092.0  15988.0  16320.0  38271.0  15793.0  16182.0   \n",
       "\n",
       "   txl11_z  txl12_x  txl12_y  txl12_z  txl13_x  txl13_y  txl13_z  txl14_x   \n",
       "0  37601.0  15631.0  16283.0  38356.0  16171.0  16215.0  38774.0  15976.0  \\\n",
       "1  37601.0  15631.0  16283.0  38356.0  16171.0  16215.0  38774.0  15976.0   \n",
       "2  37597.0  15630.0  16283.0  38347.0  16172.0  16217.0  38777.0  15975.0   \n",
       "3  37599.0  15630.0  16282.0  38353.0  16170.0  16215.0  38776.0  15975.0   \n",
       "4  37601.0  15630.0  16284.0  38350.0  16171.0  16212.0  38774.0  15974.0   \n",
       "\n",
       "   txl14_y  txl14_z  txl15_x  txl15_y  txl15_z  txl16_x  txl16_y  txl16_z  \n",
       "0  16356.0  38258.0  15887.0  16228.0  37885.0  15693.0  16427.0  38642.0  \n",
       "1  16356.0  38258.0  15887.0  16228.0  37885.0  15693.0  16427.0  38642.0  \n",
       "2  16356.0  38252.0  15888.0  16231.0  37885.0  15692.0  16429.0  38641.0  \n",
       "3  16355.0  38257.0  15888.0  16231.0  37885.0  15692.0  16429.0  38641.0  \n",
       "4  16356.0  38257.0  15887.0  16228.0  37884.0  15693.0  16426.0  38641.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_xela_allfiles = pd.read_csv('data.csv')\n",
    "pd_sliplabel_allfiles = pd.read_csv('labels.csv')\n",
    "pd_xela_allfiles.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "pd_xela_allfiles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 229651 entries, 0 to 229650\n",
      "Data columns (total 48 columns):\n",
      " #   Column   Non-Null Count   Dtype  \n",
      "---  ------   --------------   -----  \n",
      " 0   txl1_x   229651 non-null  float64\n",
      " 1   txl1_y   229651 non-null  float64\n",
      " 2   txl1_z   229651 non-null  float64\n",
      " 3   txl2_x   229651 non-null  float64\n",
      " 4   txl2_y   229651 non-null  float64\n",
      " 5   txl2_z   229651 non-null  float64\n",
      " 6   txl3_x   229651 non-null  float64\n",
      " 7   txl3_y   229651 non-null  float64\n",
      " 8   txl3_z   229651 non-null  float64\n",
      " 9   txl4_x   229651 non-null  float64\n",
      " 10  txl4_y   229651 non-null  float64\n",
      " 11  txl4_z   229651 non-null  float64\n",
      " 12  txl5_x   229651 non-null  float64\n",
      " 13  txl5_y   229651 non-null  float64\n",
      " 14  txl5_z   229651 non-null  float64\n",
      " 15  txl6_x   229651 non-null  float64\n",
      " 16  txl6_y   229651 non-null  float64\n",
      " 17  txl6_z   229651 non-null  float64\n",
      " 18  txl7_x   229651 non-null  float64\n",
      " 19  txl7_y   229651 non-null  float64\n",
      " 20  txl7_z   229651 non-null  float64\n",
      " 21  txl8_x   229651 non-null  float64\n",
      " 22  txl8_y   229651 non-null  float64\n",
      " 23  txl8_z   229651 non-null  float64\n",
      " 24  txl9_x   229651 non-null  float64\n",
      " 25  txl9_y   229651 non-null  float64\n",
      " 26  txl9_z   229651 non-null  float64\n",
      " 27  txl10_x  229651 non-null  float64\n",
      " 28  txl10_y  229651 non-null  float64\n",
      " 29  txl10_z  229651 non-null  float64\n",
      " 30  txl11_x  229651 non-null  float64\n",
      " 31  txl11_y  229651 non-null  float64\n",
      " 32  txl11_z  229651 non-null  float64\n",
      " 33  txl12_x  229651 non-null  float64\n",
      " 34  txl12_y  229651 non-null  float64\n",
      " 35  txl12_z  229651 non-null  float64\n",
      " 36  txl13_x  229651 non-null  float64\n",
      " 37  txl13_y  229651 non-null  float64\n",
      " 38  txl13_z  229651 non-null  float64\n",
      " 39  txl14_x  229651 non-null  float64\n",
      " 40  txl14_y  229651 non-null  float64\n",
      " 41  txl14_z  229651 non-null  float64\n",
      " 42  txl15_x  229651 non-null  float64\n",
      " 43  txl15_y  229651 non-null  float64\n",
      " 44  txl15_z  229651 non-null  float64\n",
      " 45  txl16_x  229651 non-null  float64\n",
      " 46  txl16_y  229651 non-null  float64\n",
      " 47  txl16_z  229651 non-null  float64\n",
      "dtypes: float64(48)\n",
      "memory usage: 84.1 MB\n"
     ]
    }
   ],
   "source": [
    "pd_xela_allfiles.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   slip\n",
       "0   0.0\n",
       "1   0.0\n",
       "2   0.0\n",
       "3   0.0\n",
       "4   0.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_sliplabel_allfiles.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "pd_sliplabel_allfiles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 229651 entries, 0 to 229650\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   slip    229651 non-null  float64\n",
      "dtypes: float64(1)\n",
      "memory usage: 1.8 MB\n"
     ]
    }
   ],
   "source": [
    "pd_sliplabel_allfiles.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_label = pd_sliplabel_allfiles.values.reshape(pd_sliplabel_allfiles.shape[0], 1)\n",
    "pd_data =  pd_xela_allfiles.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_data = torch.from_numpy(pd_data.astype(np.float32))\n",
    "pd_label = torch.from_numpy(pd_label.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into train and test\n",
    "pd_data_train, pd_data_test, pd_label_train, pd_label_test = train_test_split(pd_data, pd_label, test_size=0.1, shuffle=True)\n",
    "\n",
    "#split into train and validation\n",
    "pd_data_train, pd_data_valid, pd_label_train, pd_label_valid = train_test_split(pd_data_train, pd_label_train, test_size=0.3, shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch_Taxels(Dataset):\n",
    "\n",
    "    def __init__(self, pd_data_train, pd_label_train, pd_data_valid, pd_label_valid, valid = None):\n",
    "        self.x = pd_data_train\n",
    "        self.y = pd_label_train\n",
    "        self.xvalid = pd_data_valid\n",
    "        self.yvalid = pd_label_valid\n",
    "        self.valid = valid\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.valid == True:\n",
    "            return self.xvalid.shape[0]\n",
    "        else:\n",
    "            return self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if self.valid == True:\n",
    "            return self.xvalid[idx].to(device), self.yvalid[idx].to(device)\n",
    "        else:\n",
    "            return self.x[idx].to(device), self.y[idx].to(device)\n",
    "\n",
    "dataset = Batch_Taxels(pd_data_train, pd_label_train, pd_data_valid, pd_label_valid)\n",
    "dataset2 = Batch_Taxels(pd_data_train, pd_label_train, pd_data_valid, pd_label_valid, valid = True)\n",
    "\n",
    "xelaloader = DataLoader(dataset = dataset, batch_size=256, shuffle=True)\n",
    "xelaloadervalid = DataLoader(dataset = dataset2, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 48]) torch.Size([256, 1])\n"
     ]
    }
   ],
   "source": [
    "for i in xelaloader:\n",
    "    print(i[0].shape, i[1].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(n_classes=1, n_features=48,\n",
    "                  embed_dim=32, depth=2, n_heads=4, mlp_ratio=1., \n",
    "                  qkv_bias=True, p=0.1, attn_p=0.1, proj_p=0.1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given normalized_shape=[32], expected input with shape [*, 32], but got input of size[256, 49, 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 25\u001b[0m\n\u001b[1;32m     21\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m     22\u001b[0m \u001b[39mfor\u001b[39;00m (x, y) \u001b[39min\u001b[39;00m (xelaloader):\n\u001b[1;32m     23\u001b[0m \n\u001b[1;32m     24\u001b[0m     \u001b[39m#Forward pass\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m     y_pred \u001b[39m=\u001b[39m model(x)\n\u001b[1;32m     27\u001b[0m     \u001b[39m#compute the loss\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     l \u001b[39m=\u001b[39m criterion(y_pred, y)\n",
      "File \u001b[0;32m~/miniconda3/envs/aienv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Developer/Nelson/transformer_model.py:119\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    116\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_drop(x) \u001b[39m# dropout \u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[39mfor\u001b[39;00m block \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks:\n\u001b[0;32m--> 119\u001b[0m     x \u001b[39m=\u001b[39m block(x)\n\u001b[1;32m    121\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(x) \u001b[39m# layer normalization\u001b[39;00m\n\u001b[1;32m    122\u001b[0m cls_token_final \u001b[39m=\u001b[39m x[:, \u001b[39m0\u001b[39m] \u001b[39m# (n_samples, embed_dim)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/aienv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Developer/Nelson/transformer_model.py:82\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 82\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm1(x))\n\u001b[1;32m     83\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x))\n\u001b[1;32m     84\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/aienv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/aienv/lib/python3.10/site-packages/torch/nn/modules/normalization.py:190\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 190\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlayer_norm(\n\u001b[1;32m    191\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnormalized_shape, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps)\n",
      "File \u001b[0;32m~/miniconda3/envs/aienv/lib/python3.10/site-packages/torch/nn/functional.py:2515\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2511\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, weight, bias):\n\u001b[1;32m   2512\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2513\u001b[0m         layer_norm, (\u001b[39minput\u001b[39m, weight, bias), \u001b[39minput\u001b[39m, normalized_shape, weight\u001b[39m=\u001b[39mweight, bias\u001b[39m=\u001b[39mbias, eps\u001b[39m=\u001b[39meps\n\u001b[1;32m   2514\u001b[0m     )\n\u001b[0;32m-> 2515\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mlayer_norm(\u001b[39minput\u001b[39;49m, normalized_shape, weight, bias, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given normalized_shape=[32], expected input with shape [*, 32], but got input of size[256, 49, 1]"
     ]
    }
   ],
   "source": [
    "#Training and validation loop \n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "\n",
    "t_loss = []\n",
    "v_loss = []\n",
    "\n",
    "t_acc = []\n",
    "v_acc = []\n",
    "\n",
    "t_acc_t = []\n",
    "v_acc_t = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    #Train per batch\n",
    "    \n",
    "    total = 0\n",
    "    correct = 0\n",
    "    l_xelaloader = 0\n",
    "\n",
    "    model.train()\n",
    "    for (x, y) in (xelaloader):\n",
    "\n",
    "        #Forward pass\n",
    "        y_pred = model(x)\n",
    "       \n",
    "        #compute the loss\n",
    "        l = criterion(y_pred, y)\n",
    "\n",
    "        #empty the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #compute the gradient\n",
    "        l.backward()\n",
    "\n",
    "        #update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        #append each loss per batch\n",
    "        train_loss.append(l.item())\n",
    "\n",
    "        #accuracy\n",
    "        total += y.size(0)\n",
    "        correct += y_pred.round().eq(y).sum().item()\n",
    "        l_xelaloader += x.shape[0]\n",
    "        \n",
    "    \n",
    "    t_acc = correct/l_xelaloader\n",
    "    t_acc_t.append(t_acc)\n",
    "\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    l_xelaloader = 0\n",
    "\n",
    "    #calculate and plot the validation loss\n",
    "    model.eval()\n",
    "    for (x,y) in (xelaloadervalid):\n",
    "        y_pred_test = model(x)\n",
    "        lv = criterion(y_pred_test, y)\n",
    "        #append the loss per batch\n",
    "        valid_loss.append(lv.item())\n",
    "\n",
    "        #accuracy\n",
    "        total += y.size(0)\n",
    "        correct += y_pred_test.round().eq(y).sum().item()\n",
    "        l_xelaloader += x.shape[0]\n",
    "        \n",
    "    v_acc = correct/l_xelaloader\n",
    "    v_acc_t.append(v_acc)\n",
    "\n",
    "    #append the total loss and accuracy per epoch\n",
    "    t_loss.append(np.mean(train_loss))\n",
    "    v_loss.append(np.mean(valid_loss))\n",
    "\n",
    "    print(f'Epoch {epoch+1}, loss = {l:.8f} , val_loss = {lv:.8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'transformer.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
