{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, recall_score, precision_score \n",
    "\n",
    "# Imports\n",
    "import torch\n",
    "import torch.nn.functional as F  # Parameterless functions, like (some) activation functions\n",
    "import torchvision.datasets as datasets  # Standard datasets\n",
    "import torchvision.transforms as transforms  # Transformations we can perform on our dataset for augmentation\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the data \n",
    "#algorithm to read all the files\n",
    "\n",
    "'''\n",
    "for folder in this folder:\n",
    "    read xelasensor1.csv\n",
    "    read sliplabel.csv\n",
    "    concat it in a single dataframe along axis = 0\n",
    "\n",
    "print the dataframe\n",
    "'''\n",
    "import os\n",
    "\n",
    "directory = '/Users/elijahnelson/Desktop/SIWES/IML/Tactile_IML/train2dof'\n",
    "directory2 = '/Users/elijahnelson/Desktop/SIWES/IML/Tactile_IML/'\n",
    "\n",
    "def read_file(detect_or_pred, n = None):\n",
    "\n",
    "    #store all directories in a list\n",
    "    list_xela_allfiles = []\n",
    "    list_sliplabel_allfiles = []\n",
    "\n",
    "    for root, subdirectories, files in os.walk(directory):\n",
    "        for sdirectory in subdirectories:\n",
    "\n",
    "            #subdirectory with absolute path\n",
    "            subdirectory = '{}/{}'.format(root, sdirectory)\n",
    "\n",
    "            #read specific files in the subdirectory\n",
    "            for file in os.listdir(subdirectory):\n",
    "            \n",
    "                if file.endswith(\"sensor1.csv\"):\n",
    "                    df = pd.read_csv('{}/{}'.format(subdirectory, file), index_col=None, header=0)\n",
    "                    \n",
    "                    if detect_or_pred ==0:\n",
    "                        list_xela_allfiles.append(df)\n",
    "                    elif detect_or_pred ==1 and n is not None:\n",
    "                        list_xela_allfiles.append(df[:-n])\n",
    "\n",
    "                if file.endswith(\"label.csv\"):\n",
    "                    df = pd.read_csv('{}/{}'.format(subdirectory, file), index_col=None, header=0)\n",
    "                    if detect_or_pred ==0:\n",
    "                        list_sliplabel_allfiles.append(df)\n",
    "                    elif detect_or_pred ==1 and n is not None: \n",
    "                        list_sliplabel_allfiles.append(df[n:])\n",
    "\n",
    "    return list_xela_allfiles, list_sliplabel_allfiles\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# METRICS DEFINITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def detection_metrics(xela_test, sliplabel_test, model_type, acc = None):\n",
    "    #predict using the holdout set (DONE)\n",
    "    predicted = model_type(xela_test).detach().numpy()\n",
    "    predicted_cls = predicted.round()\n",
    "\n",
    "    #Plot the loss values against number of epochs (DONE)\n",
    "    #validation test (DONE)\n",
    "\n",
    "    #Print the accuracy\n",
    "    accuracy = accuracy_score(sliplabel_test.numpy(), predicted_cls)\n",
    "    print(f'Accuracy for slip detection is {accuracy}')\n",
    "\n",
    "    #Print the fscore\n",
    "    fscore = f1_score(sliplabel_test.numpy(), predicted_cls, average='macro')\n",
    "    print(f'Fscore for slip detection is {fscore}')\n",
    "\n",
    "    #print the Precision\n",
    "    precision = precision_score(sliplabel_test.numpy(), predicted_cls, average='macro')\n",
    "    print(f'Precision for slip detection is {precision}')\n",
    "\n",
    "    #print the Recall\n",
    "    recall = recall_score(sliplabel_test.numpy(), predicted_cls, average='macro')\n",
    "    print(f'Recall for slip detection is {recall}')\n",
    "\n",
    "def slip_metrics(n, xela_test, sliplabel_test, modeltype):\n",
    "    #predict using the holdout set (DONE)\n",
    "    predicted = modeltype(xela_test).detach().numpy()\n",
    "    predicted_cls = predicted.round()\n",
    "\n",
    "    #Plot the loss values against number of epochs (DONE)\n",
    "    #validation test (DONE)\n",
    "\n",
    "    #Print the accuracy\n",
    "    x = 0\n",
    "    for i in range(predicted_cls.shape[0]):\n",
    "        if predicted_cls[i].item() == sliplabel_test[i].item():\n",
    "            x += 1\n",
    "\n",
    "    accuracy = x/ float(sliplabel_test.shape[0])\n",
    "    print(f'Accuracy for slip prediction for (t+{n}) is {accuracy}')\n",
    "\n",
    "    #Print the fscore\n",
    "    fscore = f1_score(sliplabel_test.numpy(), predicted_cls, average='macro')\n",
    "    print(f'Fscore for slip prediction for (t+{n}) is {fscore}')\n",
    "\n",
    "    #print the Precision\n",
    "    precision = precision_score(sliplabel_test.numpy(), predicted_cls, average='macro')\n",
    "    print(f'Precision for slip prediction for (t+{n}) is {precision}')\n",
    "\n",
    "    #print the Recall\n",
    "    recall = recall_score(sliplabel_test.numpy(), predicted_cls, average='macro')\n",
    "    print(f'Recall for slip prediction for (t+{n}) is {recall}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIP DETECTION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat the list of xela_allfiles and sliplabel_allfiles across axis = 0\n",
    "list_xela_allfiles, list_sliplabel_allfiles = read_file(0)\n",
    "pd_xela_allfiles = pd.concat(list_xela_allfiles, axis=0, ignore_index=True)\n",
    "pd_sliplabel_allfiles = pd.concat(list_sliplabel_allfiles, axis=0, ignore_index=True)\n",
    "pd_sliplabel_allfiles = pd_sliplabel_allfiles['slip']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dealing with imbalanced data \n",
    "\n",
    "\n",
    "#dealing with imbalanced data using SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "# Resampling the minority class. The strategy can be changed as required.\n",
    "sm = SMOTE(sampling_strategy='minority', random_state=42)\n",
    "# Fit the model to generate the data.\n",
    "pd_xela_allfiles, pd_sliplabel_allfiles = sm.fit_resample(pd_xela_allfiles, pd_sliplabel_allfiles)\n",
    "oversampled = pd.concat([pd.DataFrame(pd_xela_allfiles), pd.DataFrame(pd_sliplabel_allfiles)], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to numpy values\n",
    "pd_xela_allfiles = np.array(pd_xela_allfiles.values)\n",
    "pd_sliplabel_allfiles = np.array(pd_sliplabel_allfiles.values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count plot of all slip labels\n",
    "sns.countplot(pd_sliplabel_allfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into train and test\n",
    "xela_train, xela_test, sliplabel_train, sliplabel_test = train_test_split(pd_xela_allfiles, pd_sliplabel_allfiles, shuffle=True, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into validation and holdout\n",
    "xela_train, xela_valid, sliplabel_train, sliplabel_valid = train_test_split(xela_train, sliplabel_train, shuffle = True, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale the data\n",
    "sc = StandardScaler()\n",
    "xela_train = sc.fit_transform(xela_train)\n",
    "xela_test = sc.transform(xela_test)\n",
    "xela_valid = sc.transform(xela_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to tensor values\n",
    "xela_train = torch.from_numpy(xela_train.astype(np.float32))\n",
    "xela_test = torch.from_numpy(xela_test.astype(np.float32))\n",
    "\n",
    "sliplabel_train = torch.from_numpy(sliplabel_train.astype(np.float32))\n",
    "sliplabel_test = torch.from_numpy(sliplabel_test.astype(np.float32))\n",
    "\n",
    "xela_valid = torch.from_numpy(xela_valid.astype(np.float32))\n",
    "\n",
    "\n",
    "sliplabel_valid = torch.from_numpy(sliplabel_valid.astype(np.float32))\n",
    "\n",
    "\n",
    "#reshape the y tensor\n",
    "sliplabel_train = sliplabel_train.view(sliplabel_train.shape[0], 1)\n",
    "sliplabel_test = sliplabel_test.view(sliplabel_test.shape[0], 1)\n",
    "\n",
    "sliplabel_valid = sliplabel_valid.view(sliplabel_valid.shape[0], 1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply batch training on the training/validation data\n",
    "class Batchdata(Dataset):\n",
    "    def __init__(self, xela_train, sliplabel_train, xela_valid, sliplabel_valid, valid = None):\n",
    "        self.x = xela_train\n",
    "        self.y = sliplabel_train\n",
    "        self.xvalid = xela_valid\n",
    "        self.yvalid = sliplabel_valid\n",
    "        self.valid = valid\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.valid == True:\n",
    "            return self.xvalid.shape[0]\n",
    "        else:\n",
    "            return self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if self.valid == True:\n",
    "            return self.xvalid[idx], self.yvalid[idx]\n",
    "        else:\n",
    "            return self.x[idx], self.y[idx]\n",
    "\n",
    "dataset = Batchdata(xela_train, sliplabel_train, xela_valid, sliplabel_valid)\n",
    "dataset2 = Batchdata(xela_train, sliplabel_train, xela_valid, sliplabel_valid, valid = True)\n",
    "\n",
    "xelaloader = DataLoader(dataset = dataset, batch_size=32, shuffle=True)\n",
    "xelaloadervalid = DataLoader(dataset = dataset2, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the NN class\n",
    "\n",
    "inputsize = xela_train.shape[1]\n",
    "outputsize = 1\n",
    "hiddensize1 = 16\n",
    "hiddensize2 = 10\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, inputsize, hiddensize1, hiddensize2, outputsize):\n",
    "        super(NeuralNet, self).__init__()\n",
    "\n",
    "        self.l1 = nn.Linear(inputsize, hiddensize1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(hiddensize1, hiddensize2)\n",
    "        self.l3 = nn.Linear(hiddensize2, outputsize)\n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.l3(out)\n",
    "        out = torch.sigmoid(out)\n",
    "        return(out)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train with batch training\n",
    "model = NeuralNet(inputsize, hiddensize1, hiddensize2, outputsize)\n",
    "\n",
    "#Construct the loss and Optimizer\n",
    "learning_rate = 0.001\n",
    "loss = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "n_iters = 100\n",
    "\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "\n",
    "t_loss = []\n",
    "v_loss = []\n",
    "\n",
    "t_acc = []\n",
    "v_acc = []\n",
    "\n",
    "t_acc_t = []\n",
    "v_acc_t = []\n",
    "\n",
    "\n",
    "#Train the model\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    #Train per batch\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    l_xelaloader = 0\n",
    "\n",
    "    model.train()\n",
    "    for (x, y) in (xelaloader):\n",
    "      \n",
    "        #empty the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #Forward pass\n",
    "        y_pred = model(x)\n",
    "\n",
    "        #compute the loss\n",
    "        l = loss(y_pred, y)\n",
    "\n",
    "        #compute the gradient\n",
    "        l.backward()\n",
    "\n",
    "        #update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        #append each loss per batch\n",
    "        train_loss.append(l.item())\n",
    "\n",
    "        #accuracy\n",
    "        total += y.size(0)\n",
    "        correct += y_pred.round().eq(y).sum().item()\n",
    "        l_xelaloader += x.shape[0]\n",
    "        \n",
    "    \n",
    "    t_acc = correct/l_xelaloader\n",
    "    t_acc_t.append(t_acc)\n",
    "\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    l_xelaloader = 0\n",
    "\n",
    "    #calculate and plot the validation loss\n",
    "    model.eval()\n",
    "    for (x,y) in (xelaloadervalid):\n",
    "        y_pred_test = model(x)\n",
    "        lv = loss(y_pred_test, y)\n",
    "\n",
    "        #append each loss per batch\n",
    "        valid_loss.append(lv.item())\n",
    "\n",
    "        #accuracy\n",
    "        total += y.size(0)\n",
    "        correct += y_pred_test.round().eq(y).sum().item()\n",
    "        l_xelaloader += x.shape[0]\n",
    "    \n",
    "    v_acc = correct/l_xelaloader\n",
    "    v_acc_t.append(v_acc)\n",
    "\n",
    "    #append the total loss and accuracy per epoch\n",
    "    t_loss.append(np.mean(train_loss))\n",
    "    v_loss.append(np.mean(valid_loss))\n",
    "\n",
    "\n",
    "    print(f'For training epoch {epoch+1}, loss ={l:.8f}', f'For validation epoch {epoch+1}, loss ={lv:.8f}'  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Plot the loss per epoch for training and validation\n",
    "\n",
    "#create a subplot of 3 plots\n",
    "\n",
    "fig, ax = plt.subplots(1,2,figsize=(15,5))\n",
    "\n",
    "#set labels\n",
    "ax[0].set_xlabel('Epochs', color = 'black')\n",
    "ax[0].set_ylabel('Loss', color = 'black')\n",
    "ax[1].set_xlabel('Epochs', color = 'black')\n",
    "ax[1].set_ylabel('Accuracy', color = 'black')\n",
    "\n",
    "\n",
    "ax[0].plot(t_loss, label='Train Loss', color='red', linewidth=2, linestyle='dashed')\n",
    "ax[0].plot(v_loss, label='Validation Loss', color = 'blue')\n",
    "ax[1].plot(t_acc_t, label = 'Train Accuracy', color = 'orange')\n",
    "ax[1].plot(v_acc_t, label = 'Validation Accuracy', color = 'green')\n",
    "\n",
    "#set legends\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metrics after Batch Training\n",
    "print('Metrics after Batch Training')\n",
    "detection_metrics(xela_valid, sliplabel_valid, model_type = model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIP PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat the list of xela_allfiles and sliplabel_allfiles across axis = 0\n",
    "n = 20 #-> Define the number of time steps for prediction\n",
    "list_xela_allfiles, list_sliplabel_allfiles = read_file(1, n)\n",
    "pd_xela_allfiles = pd.concat(list_xela_allfiles, axis=0, ignore_index=True)\n",
    "pd_sliplabel_allfiles = pd.concat(list_sliplabel_allfiles, axis=0, ignore_index=True)\n",
    "pd_sliplabel_allfiles = pd_sliplabel_allfiles['slip']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dealing with imbalanced data using SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "# Resampling the minority class. The strategy can be changed as required.\n",
    "sm = SMOTE(sampling_strategy='minority', random_state=42)\n",
    "# Fit the model to generate the data.\n",
    "pd_xela_allfiles, pd_sliplabel_allfiles = sm.fit_resample(pd_xela_allfiles, pd_sliplabel_allfiles)\n",
    "oversampled = pd.concat([pd.DataFrame(pd_xela_allfiles), pd.DataFrame(pd_sliplabel_allfiles)], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to numpy values\n",
    "pd_xela_allfiles = np.array(pd_xela_allfiles.values)\n",
    "pd_sliplabel_allfiles = np.array(pd_sliplabel_allfiles.values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into train and validation\n",
    "xela_train, xela_test, sliplabel_train, sliplabel_test = train_test_split(pd_xela_allfiles, pd_sliplabel_allfiles, shuffle=True, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into validation and holdout\n",
    "xela_train, xela_valid, sliplabel_train, sliplabel_valid = train_test_split(xela_train, sliplabel_train, shuffle = True, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale the data\n",
    "sc = StandardScaler()\n",
    "xela_train = sc.fit_transform(xela_train)\n",
    "xela_test = sc.transform(xela_test)\n",
    "xela_valid = sc.transform(xela_valid)\n",
    "\n",
    "#convert to tensor values\n",
    "xela_train = torch.from_numpy(xela_train.astype(np.float32))\n",
    "xela_test = torch.from_numpy(xela_test.astype(np.float32))\n",
    "\n",
    "sliplabel_train = torch.from_numpy(sliplabel_train.astype(np.float32))\n",
    "sliplabel_test = torch.from_numpy(sliplabel_test.astype(np.float32))\n",
    "\n",
    "xela_valid = torch.from_numpy(xela_valid.astype(np.float32))\n",
    "\n",
    "\n",
    "sliplabel_valid = torch.from_numpy(sliplabel_valid.astype(np.float32))\n",
    "\n",
    "\n",
    "#reshape the y tensor\n",
    "sliplabel_train = sliplabel_train.view(sliplabel_train.shape[0], 1)\n",
    "sliplabel_test = sliplabel_test.view(sliplabel_test.shape[0], 1)\n",
    "\n",
    "sliplabel_valid = sliplabel_valid.view(sliplabel_valid.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply batch division on the training/validation data\n",
    "class Batchdata(Dataset):\n",
    "    def __init__(self, xela_train, sliplabel_train, xela_valid, sliplabel_valid, valid = None):\n",
    "        self.x = xela_train\n",
    "        self.y = sliplabel_train\n",
    "        self.xvalid = xela_valid\n",
    "        self.yvalid = sliplabel_valid\n",
    "        self.valid = valid\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.valid == True:\n",
    "            return self.xvalid.shape[0]\n",
    "        else:\n",
    "            return self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if self.valid == True:\n",
    "            return self.xvalid[idx], self.yvalid[idx]\n",
    "        else:\n",
    "            return self.x[idx], self.y[idx]\n",
    "\n",
    "\n",
    "#DetAILS FOR BATCH Training\n",
    "dataset = Batchdata(xela_train, sliplabel_train, xela_valid, sliplabel_valid)\n",
    "dataset2 = Batchdata(xela_train, sliplabel_train, xela_valid, sliplabel_valid, valid = True)\n",
    "\n",
    "xelaloader = DataLoader(dataset = dataset, batch_size=32, shuffle=True)\n",
    "xelaloadervalid = DataLoader(dataset = dataset2, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the model parameters\n",
    "inputsize = xela_train.shape[1]\n",
    "outputsize = 1\n",
    "hiddensize1 = 16\n",
    "hiddensize2 = 10\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, inputsize, hiddensize1, hiddensize2, outputsize):\n",
    "        super(NeuralNet, self).__init__()\n",
    "\n",
    "        self.l1 = nn.Linear(inputsize, hiddensize1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(hiddensize1, hiddensize2)\n",
    "        self.l3 = nn.Linear(hiddensize2, outputsize)\n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.l3(out)\n",
    "        out = torch.sigmoid(out)\n",
    "        return(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train with batch training\n",
    "model = NeuralNet(inputsize, hiddensize1, hiddensize2, outputsize)\n",
    "\n",
    "#Construct the loss and Optimizer\n",
    "learning_rate = 0.01\n",
    "loss = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "n_iters = 100\n",
    "\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "\n",
    "t_loss = []\n",
    "v_loss = []\n",
    "\n",
    "t_acc = []\n",
    "v_acc = []\n",
    "\n",
    "t_acc_t = []\n",
    "v_acc_t = []\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "l_xelaloader = 0\n",
    "\n",
    "#Train the model\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    #Train per batch\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    l_xelaloader = 0\n",
    "    \n",
    "    model.train()\n",
    "    for (x, y) in (xelaloader):\n",
    "      \n",
    "        #empty the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #Forward pass\n",
    "        y_pred = model(x)\n",
    "\n",
    "        #compute the loss\n",
    "        l = loss(y_pred, y)\n",
    "\n",
    "        #compute the gradient\n",
    "        l.backward()\n",
    "\n",
    "        #update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        #append each loss per batch\n",
    "        train_loss.append(l.item())\n",
    "\n",
    "        #accuracy\n",
    "        total += y.size(0)\n",
    "        correct += y_pred.round().eq(y).sum().item()\n",
    "        l_xelaloader += x.shape[0]\n",
    "     \n",
    "    \n",
    "    t_acc = correct/l_xelaloader\n",
    "    t_acc_t.append(t_acc)\n",
    "\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    l_xelaloader = 0\n",
    "\n",
    "    #calculate and plot the validation loss\n",
    "    model.eval()\n",
    "    for (x,y) in (xelaloadervalid):\n",
    "        y_pred_test = model(x)\n",
    "        lv = loss(y_pred_test, y)\n",
    "\n",
    "        #append each loss per batch\n",
    "        valid_loss.append(lv.item())\n",
    "\n",
    "        #accuracy\n",
    "        total += y.size(0)\n",
    "        correct += y_pred_test.round().eq(y).sum().item()\n",
    "        l_xelaloader += x.shape[0]\n",
    "    \n",
    "    v_acc = correct/l_xelaloader\n",
    "    v_acc_t.append(v_acc)\n",
    "\n",
    "    #append the total loss and accuracy per epoch\n",
    "    t_loss.append(np.mean(train_loss))\n",
    "    \n",
    "\n",
    "    v_loss.append(np.mean(valid_loss))\n",
    "\n",
    "\n",
    "    print(f'For training epoch {epoch+1}, loss ={l:.8f}', f'For validation epoch {epoch+1}, loss ={lv:.8f}'  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Plot the loss per epoch for training and validation\n",
    "\n",
    "#create a subplot of 3 plots\n",
    "\n",
    "fig, ax = plt.subplots(1,2,figsize=(15, 5))\n",
    "\n",
    "#set labels\n",
    "\n",
    "ax[0].set_xlabel('Epochs', color = 'black')\n",
    "ax[0].set_ylabel('Loss', color = 'black')\n",
    "ax[1].set_xlabel('Epochs', color = 'black')\n",
    "ax[1].set_ylabel('Loss', color = 'black')\n",
    "\n",
    "ax[0].plot(t_loss, label='Train Loss', color='red', linewidth=2, linestyle='dashed')\n",
    "ax[0].plot(v_loss, label='Validation Loss', color = 'blue')\n",
    "ax[1].plot(t_acc_t, label = 'Train Accuracy', color = 'orange')\n",
    "ax[1].plot(v_acc_t, label = 'Validation Accuracy', color = 'green')\n",
    "\n",
    "#set legends\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metrics after batch training for slip prediction\n",
    "print('Metrics after batch training for slip prediction')\n",
    "slip_metrics(n, xela_test, sliplabel_test, modeltype = model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('nei')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 06:04:14) \n[Clang 12.0.1 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "173c0cb41f479ae2d1f90bf66f9ae3aceca0c8feada6413b4ebace4131a19a6b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
